{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a784045-6498-458e-83da-e7b2caac9ebd",
   "metadata": {},
   "source": [
    "# For one source scrape google scholar to get a list of all citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fcf26c",
   "metadata": {},
   "source": [
    "To run the code replace \"base_url\" and \"filename\".\n",
    "- \"base_url\" is the url of the google scholar page you want to scrape.\n",
    "\n",
    "*Note:* It should start with \"https://scholar.google.com/scholar?start=0\" . You get this url by going to page 2 and then back to page 1 by clicking on the \"previous\" button.\n",
    "- \"filename\" is the name of the json file you want to save the data to.\n",
    "\n",
    "\n",
    "Disclaimer:\n",
    "- Scholar does not like it when you scrape their website so it might block you after a while so don't run it more than needed in one day or so. If you are blocked, try a vpn or wait some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1166ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "510683b6-3d5d-4059-a1de-cf3d80dddb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nextpage(soup):\n",
    "    \"\"\"Get the next page URL from the bottom navigation bar\"\"\"\n",
    "    bottom_nav = soup.find(\"div\", id='gs_n')\n",
    "    next_link = bottom_nav.find_all('a')\n",
    "    if next_link is None:\n",
    "        return None\n",
    "    if len(next_link) > 0 and \"Next\" in next_link[-1].find(\"b\"):\n",
    "        next_link = next_link[-1]\n",
    "        part_url = next_link['href']\n",
    "        url = \"https://scholar.google.com\" +part_url\n",
    "        return url\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "266c5d08-5a0f-42b5-83d6-8aacc6ad46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nextpage_url(url):\n",
    "    \"\"\" Get the next page URL by adding 10 to the current page number in the URL\"\"\"\n",
    "    # Search for the number in the URL using regular expression\n",
    "    match = re.search(r'\\d+', url)\n",
    "\n",
    "    if match:\n",
    "        # Extract the matched number\n",
    "        current_number = int(match.group())\n",
    "\n",
    "        # Update the number by adding 10\n",
    "        new_number = current_number + 10\n",
    "\n",
    "        # Replace the old number with the new number in the URL\n",
    "        updated_url = re.sub(r'\\d+', str(new_number), url, count=1)\n",
    "        print(updated_url)\n",
    "        return updated_url\n",
    "    else:\n",
    "        # If no number is found in the URL, return the original URL\n",
    "        print('Errooor: could not find a number in the URL')\n",
    "        return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c940e73-24db-4e60-b88b-7084faa7e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_year(input_string):\n",
    "    \"\"\"Find the last year in the input string\"\"\"\n",
    "    # Define a regex pattern to match a four-digit year\n",
    "    year_pattern = re.compile(r'\\b\\d{4}\\b')\n",
    "\n",
    "    # Find all matches of the pattern in the input string\n",
    "    matches = year_pattern.findall(input_string)\n",
    "\n",
    "    # If there are no matches, return None\n",
    "    if not matches:\n",
    "        return None\n",
    "\n",
    "    # Return the last match found\n",
    "    return int(matches[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e42a099f-8791-4a16-962e-ddbc0275f3c6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "def get_citations_per_page(base_url):\n",
    "    \"\"\"Get the all citations from the current page and return next page URL\"\"\"\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        citation_list = []\n",
    "        \n",
    "        for citation in soup.find_all('div', class_='gs_r gs_or gs_scl'):\n",
    "            title = citation.find('h3', class_='gs_rt').text.strip()\n",
    "\n",
    "            # Get authors\n",
    "            authors_tag = citation.find('div', class_='gs_a')\n",
    "            authors = authors_tag.text.strip() if authors_tag else ''\n",
    "            authors = [author.text +\", \" for author in  authors_tag.find_all('a') ]\n",
    "            authors = \" \".join(authors)[:-2]\n",
    "\n",
    "            # Get metdata, year and link to paper PDF\n",
    "            metadata = [content for content in authors_tag.contents if not content.name == 'a'][-1]\n",
    "            year = find_last_year(metadata)\n",
    "            link_paper = citation.find('div', class_='gs_or_ggsm')\n",
    "            link_paper = link_paper.find('a')['href'] if link_paper is not None else None\n",
    "\n",
    "            # Get the number of citations\n",
    "            number_cit_line = citation.find('a', string=lambda text: text and text.startswith(\"Cited by\"))\n",
    "            number_of_citations = None\n",
    "            if number_cit_line is not None:\n",
    "                number_of_citations = int(number_cit_line.text.replace(\"Cited by\", \"\").replace(\" \", \"\"))\n",
    "\n",
    "\n",
    "            citation_list.append({\n",
    "                'title': title,\n",
    "                'authors': authors,\n",
    "                'year': year,\n",
    "                'NumberOfCitations': number_of_citations,\n",
    "                'paper_link':link_paper,\n",
    "                'metadata':metadata\n",
    "            })\n",
    "        next_page_url = get_nextpage_url(base_url)\n",
    "        return citation_list, next_page_url\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None, None\n",
    "    \n",
    "    \n",
    "def get_citations_multipage(start_url):\n",
    "    \"\"\"Get all citations from all pages - go to next page till invalid page is reached\"\"\"\n",
    "    citation_list = []\n",
    "    \n",
    "    page_url = start_url\n",
    "    page_idx = 0\n",
    "    while True:\n",
    "        page_idx +=1\n",
    "        print(\"Processing page:\", page_idx, \"Total citations:\", len(citation_list))\n",
    "        citation_list1, next_page_url = get_citations_per_page(page_url)\n",
    "\n",
    "        if next_page_url is None or len(citation_list1) == 0:\n",
    "            print(\"Quiting No more data\")\n",
    "            return citation_list\n",
    "        \n",
    "        citation_list.extend(citation_list1)\n",
    "        page_url = next_page_url\n",
    "        time.sleep(1)\n",
    "        \n",
    "        \n",
    "    \n",
    "def store_citation_list(citation_list, filename):\n",
    "    citation_dict = {}\n",
    "    for i, item in enumerate(citation_list):\n",
    "        citation_dict[i] = item\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(citation_dict, f)\n",
    "        \n",
    "    return citation_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extracts all citations in a List, then store it as a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25a41708",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://scholar.google.com/scholar?start=0&hl=nl&as_sdt=2005&cites=10005508259730753239&scipsc=\"\n",
    "filename = \"citations_Biographynet\"\n",
    "\n",
    "\n",
    "citation_list = get_citations_multipage(base_url)\n",
    "citation_dict = store_citation_list(citation_list, filename)\n",
    "\n",
    "print_citations = True\n",
    "if print_citations:\n",
    "    if citation_list:\n",
    "        for citation_i in citation_list:\n",
    "            for k,v in citation_i.items():\n",
    "                print(f'{k} : {v}')\n",
    "            print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3890fe",
   "metadata": {},
   "source": [
    "### 2. Convert the Json to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df643dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'citations_Biographynet'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the citation list from file  -- Uncomment if you want to load the citation list from file\n",
    "# filename = \"cit_BiasBios.json\"\n",
    "# with open(filename, 'r') as f:\n",
    "#     citation_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77eb1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_excel(json_data, excel_file_path):\n",
    "    if \"xlsx\" not in excel_file_path:\n",
    "        excel_file_path += \".xlsx\"\n",
    "    # Convert the JSON data to a DataFrame\n",
    "    df = pd.DataFrame.from_dict(json_data, orient='index')\n",
    "\n",
    "    # Write the DataFrame to an Excel file\n",
    "    df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "\n",
    "excel_file_path = filename + \".xlsx\"  # Provide the desired output Excel file path\n",
    "json_to_excel(citation_dict, excel_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e292dfb4-b178-44fe-b6c3-9d03ec618aa9",
   "metadata": {},
   "source": [
    "### 3. Get Abstract from Arxiv and dl.acm. Add to json and store as excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb879d41-b280-4a4a-882b-5a9e1729d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number_from_url(url):\n",
    "    # Define the regular expression pattern\n",
    "    pattern = r'/(\\d+\\.\\d+)(?:\\.pdf)?'\n",
    "\n",
    "    # Use re.search to find the pattern in the URL\n",
    "    match = re.search(pattern, url)\n",
    "\n",
    "    # Check if a match is found and extract the number\n",
    "    if match:\n",
    "        extracted_number = match.group(1)\n",
    "        return extracted_number\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_abs_arxiv(arx_url):\n",
    "    response = requests.get(arx_url)\n",
    "    abstract_text = None\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for abstract in soup.find_all('blockquote', class_='abstract mathjax'):\n",
    "            abstract_text = abstract.text.replace(\"Abstract:\", \"\").replace(\"\\n\", \"\")            \n",
    "    else:\n",
    "        print(\"URL COULDN'T BE LOADED\")\n",
    "        \n",
    "    return abstract_text\n",
    "\n",
    "\n",
    "def get_abs_acm(acm_url):\n",
    "    response = requests.get(acm_url)\n",
    "    \n",
    "    abstract_text = None\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for abstract in soup.find_all('div', class_='abstractSection abstractInFull'):\n",
    "            abstract_text = abstract.text.replace(\"\\n\", \"\")\n",
    "    else:\n",
    "        print(\"URL COULDN'T BE LOADED\")\n",
    "    if abstract_text is None:\n",
    "        print(\"could not load abstract for:\", acm_url)\n",
    "    return abstract_text\n",
    "\n",
    "\n",
    "def add_abstracts_to_citation_dict(citation_dict):\n",
    "    \"\"\"Add the abstracts to the citation_dict\"\"\"\n",
    "    linklist = []\n",
    "    for _, cit in citation_dict.items():\n",
    "        paperlink = cit[\"paper_link\"]\n",
    "        abstract_text = None\n",
    "        if paperlink is not None:\n",
    "            if \"arxiv\" in paperlink:\n",
    "                print(\"Arxiv link found\")\n",
    "                number = extract_number_from_url(paperlink)\n",
    "                url = \"https://arxiv.org/abs/\" + str(number)\n",
    "                linklist.append(number)\n",
    "                abstract_text = get_abs_arxiv(url)\n",
    "                time.sleep(0.2) # sleep to not overload the server\n",
    "                print(\"URL:\", url, \" \\n Abstract:\",abstract_text )\n",
    "            elif \"dl.acm\" in paperlink:\n",
    "                print(\"ACM link found\")\n",
    "                number = paperlink.split(\"/\")[-1]\n",
    "                url = \"https://dl.acm.org/doi/10.1145/\" + str(number)\n",
    "                abstract_text = get_abs_acm(url)\n",
    "                linklist.append(number)\n",
    "                time.sleep(0.2) # sleep to not overload the server\n",
    "                print(\"URL:\", url, \" \\n Abstract:\",abstract_text )\n",
    "\n",
    "        \n",
    "        # if abstract_text is not None:\n",
    "        cit[\"abstract\"] = abstract_text\n",
    "    print(\"Number of found abstracts:\", len(linklist))\n",
    "    return citation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "685988da-95d0-4f45-9f57-07eba414c9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arxiv link found\n",
      "URL: https://arxiv.org/abs/2306.09505  \n",
      " Abstract: Biographical event detection is a relevant task for the exploration and comparison of the ways in which people's lives are told and represented. In this sense, it may support several applications in digital humanities and in works aimed at exploring bias about minoritized groups. Despite that, there are no corpora and models specifically designed for this task. In this paper we fill this gap by presenting a new corpus annotated for biographical event detection. The corpus, which includes 20 Wikipedia biographies, was compared with five existing corpora to train a model for the biographical event detection task. The model was able to detect all mentions of the target-entity in a biography with an F-score of 0.808 and the entity-related events with an F-score of 0.859. Finally, the model was used for performing an analysis of biases about women and non-Western people in Wikipedia biographies.    \n",
      "Number of found abstracts: 1\n"
     ]
    }
   ],
   "source": [
    "citation_dict = add_abstracts_to_citation_dict(citation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc6e3d6c-bec1-47d0-91ca-d294497ccfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store json and excel file\n",
    "filename2 = filename + \"_with_abstracts\"\n",
    "with open(filename2+ \".json\", 'w') as f:\n",
    "    json.dump(citation_dict, f)\n",
    "\n",
    "json_to_excel(citation_dict, filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4680e1ee-a235-4a86-b250-b1c82a709f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_genderbias",
   "language": "python",
   "name": "rl_genderbias"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
