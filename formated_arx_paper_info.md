
 ### Non arXiv URLs: 

 - https://gist.github.com/pierrejoubert73/902cc94d79424356a8d20be2b382e1ab
 ## ArXiv items: 

###  [Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models - A. Ghandeharioun (2024)](http://arxiv.org/abs/2401.06102v2)

<details open>
<summary>Abstract</summary>

> Inspecting the information encoded in hidden representations of large
language models (LLMs) can explain models' behavior and verify their alignment
with human values. Given the capabilities of LLMs in generating
human-understandable text, we propose leveraging the model itself to explain
its internal representations in natural language. We introduce a framework
called Patchscopes and show how it can be used to answer a wide range of
questions about an LLM's computation. We show that prior interpretability
methods based on projecting representations into the vocabulary space and
intervening on the LLM computation can be viewed as instances of this
framework. Moreover, several of their shortcomings such as failure in
inspecting early layers or lack of expressivity can be mitigated by
Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also
opens up new possibilities such as using a more capable model to explain the
representations of a smaller model, and unlocks new applications such as
self-correction in multi-hop reasoning.

</details> 

###  [Attention is all you need for boosting graph convolutional neural network - Y. Wu (2024)](http://arxiv.org/abs/2403.15419v1)

<details open>
<summary>Abstract</summary>

> Graph Convolutional Neural Networks (GCNs) possess strong capabilities for
processing graph data in non-grid domains. They can capture the topological
logical structure and node features in graphs and integrate them into nodes'
final representations. GCNs have been extensively studied in various fields,
such as recommendation systems, social networks, and protein molecular
structures. With the increasing application of graph neural networks, research
has focused on improving their performance while compressing their size. In
this work, a plug-in module named Graph Knowledge Enhancement and Distillation
Module (GKEDM) is proposed. GKEDM can enhance node representations and improve
the performance of GCNs by extracting and aggregating graph information via
multi-head attention mechanism. Furthermore, GKEDM can serve as an auxiliary
transferor for knowledge distillation. With a specially designed attention
distillation method, GKEDM can distill the knowledge of large teacher models
into high-performance and compact student models. Experiments on multiple
datasets demonstrate that GKEDM can significantly improve the performance of
various GCNs with minimal overhead. Furthermore, it can efficiently transfer
distilled knowledge from large teacher networks to small student networks via
attention distillation.

</details> 



---

