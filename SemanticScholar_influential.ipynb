{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7935bf22",
   "metadata": {},
   "source": [
    "## Code to get most influential references or citations\n",
    "- Documentation of API: https://api.semanticscholar.org/api-docs/#tag/Author-Data/operation/post_graph_get_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a946197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2db2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = \"b9b220b485d2add79118ffdc2aaa148b67fa53ef\" \n",
    "\n",
    "\n",
    "url_string = f\"https://api.semanticscholar.org/graph/v1/paper/{paper_id}/citations?fields=title,authors,isInfluential\"\n",
    "\n",
    "\n",
    "# function to load the json file that is on the url\n",
    "def load_json_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return data\n",
    "\n",
    "data = load_json_from_url(url_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "180ec12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_paper_refs_or_cits(paper_id, fields=\"title,authors,isInfluential\", limit=100, use_references=True):\n",
    "    \"\"\"\n",
    "    Fetch all references for a paper, handling pagination.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: The Semantic Scholar paper ID\n",
    "        fields: Comma-separated fields to include in the response\n",
    "        limit: Number of items to fetch per request\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (all_references, influential_references)\n",
    "    \"\"\"\n",
    "\n",
    "    suffix = \"references\" if use_references else \"citations\"\n",
    "    base_url = f\"https://api.semanticscholar.org/graph/v1/paper/{paper_id}/{suffix}\"\n",
    "    url = f\"{base_url}?fields={fields}&limit={limit}\"\n",
    "    \n",
    "    all_references = []\n",
    "    offset = 0\n",
    "    \n",
    "    while True:\n",
    "        # Add offset to URL if not first request\n",
    "        paginated_url = f\"{url}&offset={offset}\" if offset > 0 else url\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(paginated_url)\n",
    "            response.raise_for_status()  # Raise exception for HTTP errors\n",
    "            data = response.json()\n",
    "            \n",
    "            # Add fetched references to our collection\n",
    "            references = data.get('data', [])\n",
    "            all_references.extend(references)\n",
    "            \n",
    "            # Check if there are more references to fetch\n",
    "            if 'next' in data:\n",
    "                offset = data['next']\n",
    "                # Add a small delay to avoid hitting rate limits\n",
    "                time.sleep(0.5)\n",
    "            else:\n",
    "                # No more pages, we're done\n",
    "                break\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching references: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Filter for influential references\n",
    "    influential_references = [ref for ref in all_references if ref.get('isInfluential', False)]\n",
    "    \n",
    "\n",
    "    print(f\"Total references: {len(all_references)}\")\n",
    "    print(f\"Influential references: {len(influential_references)}\")\n",
    "    return all_references, influential_references\n",
    "\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "# use_references = False\n",
    "\n",
    "# paper_id = \"b9b220b485d2add79118ffdc2aaa148b67fa53ef\"\n",
    "# all_refs, influential_refs = get_paper_refs_or_cits(\n",
    "#     paper_id,\n",
    "#     fields=\"title,authors,isInfluential,url\",\n",
    "#     use_references=use_references\n",
    "# )\n",
    "\n",
    "# print(f\"Total references: {len(all_refs)}\")\n",
    "# print(f\"Influential references: {len(influential_refs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec9e27b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_influential_references(influential_refs, use_references=False, limit=5):\n",
    "    \"\"\"\n",
    "    Prints the first few influential references or citations.\n",
    "\n",
    "    Args:\n",
    "        influential_refs: List of influential references or citations.\n",
    "        use_references: Boolean indicating whether to use references or citations.\n",
    "        limit: Number of influential references to print.\n",
    "    \"\"\"\n",
    "    for i, ref in enumerate(influential_refs[:limit]):\n",
    "        print(f\"\\nInfluential Reference {i+1}:\")\n",
    "        if use_references:\n",
    "            paper = ref.get('citedPaper', {})\n",
    "        else:\n",
    "            paper = ref.get('citingPaper', {})\n",
    "\n",
    "        print(f\"Title: {paper.get('title')} - (year: {paper.get('year', 'N/A')})\")\n",
    "        print(f\"Authors: {', '.join(a.get('name', '') for a in paper.get('authors', []))}\")\n",
    "        print(f\"URL: {paper.get('url')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec1ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = \"b9b220b485d2add79118ffdc2aaa148b67fa53ef\"\n",
    "\n",
    "# Machiavelli paper\n",
    "paper_id = \"5da2d404d789aeff266b63a760d07fe8bc31ba23\"\n",
    "\n",
    "# Reward Rational Implicit Choice\n",
    "paper_id = \"1f52deff193c7c3dfc77c48cbdc653c94f093a92\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b63b5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total references: 184\n",
      "Influential references: 15\n"
     ]
    }
   ],
   "source": [
    "use_references = False  # If False, it will fetch citations, otherwise references\n",
    "\n",
    "all_refs, influential_refs = get_paper_refs_or_cits(\n",
    "    paper_id,\n",
    "    fields=\"title,authors,isInfluential,url,contexts,intents,year\",\n",
    "    use_references=use_references\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99e34e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Influential Reference 1:\n",
      "Title: Learning from Preferences and Mixed Demonstrations in General Settings - (year: 2025)\n",
      "Authors: Jason R Brown, Carl Henrik Ek, Robert D Mullins\n",
      "URL: https://www.semanticscholar.org/paper/72329bf059a3ae7d5e7d0bc21a2beed31141205f\n",
      "\n",
      "Influential Reference 2:\n",
      "Title: On the Partial Identifiability in Reward Learning: Choosing the Best Reward - (year: 2025)\n",
      "Authors: Filippo Lazzati, Alberto Maria Metelli\n",
      "URL: https://www.semanticscholar.org/paper/0475eed79b448112d91f2799a4ec7d273223c087\n",
      "\n",
      "Influential Reference 3:\n",
      "Title: Learning Human Preferences Over Robot Behavior as Soft Planning Constraints - (year: 2024)\n",
      "Authors: Austin Narcomey, Nathan Tsoi, Ruta Desai, Marynel Vázquez\n",
      "URL: https://www.semanticscholar.org/paper/14ba38ecdd58366582754507b5d61465d8d6aff9\n",
      "\n",
      "Influential Reference 4:\n",
      "Title: Data Quality in Imitation Learning - (year: 2023)\n",
      "Authors: Suneel Belkhale, Yuchen Cui, Dorsa Sadigh\n",
      "URL: https://www.semanticscholar.org/paper/a91b821a95ccd417e0f1315247732dd4bcf45991\n",
      "\n",
      "Influential Reference 5:\n",
      "Title: Reward Learning With Intractable Normalizing Functions - (year: 2023)\n",
      "Authors: Joshua Hoegerman, Dylan P. Losey\n",
      "URL: https://www.semanticscholar.org/paper/4d9ad31e9e2c5040cda1adab12e80677dfdc77f2\n",
      "\n",
      "Influential Reference 6:\n",
      "Title: Active Reward Learning from Multiple Teachers - (year: 2023)\n",
      "Authors: Peter Barnett, Rachel Freedman, Justin Svegliato, Stuart J. Russell\n",
      "URL: https://www.semanticscholar.org/paper/c28af43206867cb5529164e3dd6d9ea8b7cfe7f2\n",
      "\n",
      "Influential Reference 7:\n",
      "Title: The Effect of Modeling Human Rationality Level on Learning Rewards from Multiple Feedback Types - (year: 2022)\n",
      "Authors: Gaurav R. Ghosal, M. Zurek, Daniel S. Brown, A. Dragan\n",
      "URL: https://www.semanticscholar.org/paper/e920f426eb32e64474b2a1176d97725f875dd82a\n",
      "\n",
      "Influential Reference 8:\n",
      "Title: Unified Learning from Demonstrations, Corrections, and Preferences during Physical Human–Robot Interaction - (year: 2022)\n",
      "Authors: Shaunak A. Mehta, Dylan P. Losey\n",
      "URL: https://www.semanticscholar.org/paper/0503229b89eb4611ac4a0d904541c7e2e3bf0ee2\n",
      "\n",
      "Influential Reference 9:\n",
      "Title: How to talk so AI will learn: Instructions, descriptions, and autonomy - (year: 2022)\n",
      "Authors: T. Sumers, Robert D. Hawkins, Mark K. Ho, T. Griffiths, Dylan Hadfield-Menell\n",
      "URL: https://www.semanticscholar.org/paper/9c9d9b026a9497e73624912aa9ea905d70e7d470\n",
      "\n",
      "Influential Reference 10:\n",
      "Title: Learning Submodular Objectives for Team Environmental Monitoring - (year: 2021)\n",
      "Authors: Nils Wilde, Armin Sadeghi, Stephen L. Smith\n",
      "URL: https://www.semanticscholar.org/paper/6946ea2d6565296b2ebf8ebc6e59a6385618f2e1\n",
      "\n",
      "Influential Reference 11:\n",
      "Title: Understanding the Relationship between Interactions and Outcomes in Human-in-the-Loop Machine Learning - (year: 2021)\n",
      "Authors: Yuchen Cui, Pallavi Koppol, H. Admoni, R. Simmons, Aaron Steinfeld, Tesca Fitzgerald\n",
      "URL: https://www.semanticscholar.org/paper/2f516ddbbf43c69742015a155b28ed65bbdc2880\n",
      "\n",
      "Influential Reference 12:\n",
      "Title: Optimizing Interactive Systems via Data-Driven Objectives - (year: 2020)\n",
      "Authors: Ziming Li, A. Grotov, Julia Kiseleva, M. de Rijke, Harrie Oosterhuis\n",
      "URL: https://www.semanticscholar.org/paper/b21dcffbc50ce121d54cb3d3931f543e5789f33c\n",
      "\n",
      "Influential Reference 13:\n",
      "Title: When Your AIs Deceive You: Challenges with Partial Observability of Human Evaluators in Reward Learning - (year: 2024)\n",
      "Authors: Leon Lang, Davis Foote, Stuart J. Russell, Anca Dragan, Erik Jenner, Scott Emmons\n",
      "URL: https://www.semanticscholar.org/paper/aae77645ecf5b53be3df128c3919c94bcbd02f28\n",
      "\n",
      "Influential Reference 14:\n",
      "Title: Adversarial Imitation Learning with Preferences - (year: 2023)\n",
      "Authors: Aleksandar Taranovic, A. Kupcsik, Niklas Freymuth, G. Neumann\n",
      "URL: https://www.semanticscholar.org/paper/74e3b120e375e2fa98bbee0241f6ed3eeec75a3c\n",
      "\n",
      "Influential Reference 15:\n",
      "Title: How to talk so your AI will learn: Instructions, descriptions, and autonomy - (year: 2022)\n",
      "Authors: T. Sumers, Robert D Hawkins, Mark K. Ho, T. Griffiths, Dylan Hadfield-Menell\n",
      "URL: https://www.semanticscholar.org/paper/5aa85e72597e1cc3b851af007af30124b0ca5b59\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "print_influential_references(influential_refs, use_references=use_references, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "850066da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total references with at least 2 contexts: 51\n"
     ]
    }
   ],
   "source": [
    "min_contexts = 2\n",
    "subset_refs = []\n",
    "\n",
    "for ref in all_refs:\n",
    "    contexts = ref.get('contexts', [])\n",
    "    if len(contexts) >= min_contexts:\n",
    "        subset_refs.append(ref)\n",
    "        # print(f\"Title: {ref.get('title')}\")\n",
    "\n",
    "print(f\"Total references with at least {min_contexts} contexts: {len(subset_refs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b089a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_refs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b78757c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'isInfluential': True,\n",
       " 'contexts': ['Jeon et al. (2020) interpret many of types of feedback as part of an overarching formalism, reward-rational (implicit) choice (RRC), providing a mathematical theory for reward learning that combines different types of feedback.',\n",
       "  'Reward-rational preference orderings over observations, the basis of LEOPARD, are a generalisation of the deterministic reward-rational choice framework (Jeon et al., 2020), but offer several distinct advantages.',\n",
       "  'First, we develop a general mathematical framework, reward-rational partial orderings (RRPO), extending that of deterministic reward-rational choice (RRC, Jeon et al. (2020)).'],\n",
       " 'intents': [],\n",
       " 'citingPaper': {'paperId': '72329bf059a3ae7d5e7d0bc21a2beed31141205f',\n",
       "  'url': 'https://www.semanticscholar.org/paper/72329bf059a3ae7d5e7d0bc21a2beed31141205f',\n",
       "  'title': 'Learning from Preferences and Mixed Demonstrations in General Settings',\n",
       "  'year': 2025,\n",
       "  'authors': [{'authorId': '2376282484', 'name': 'Jason R Brown'},\n",
       "   {'authorId': '2324782232', 'name': 'Carl Henrik Ek'},\n",
       "   {'authorId': '2376265247', 'name': 'Robert D Mullins'}]}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_refs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9845087d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Title: Learning from Preferences and Mixed Demonstrations in General Settings - year: 2025\n",
      "URL: https://www.semanticscholar.org/paper/72329bf059a3ae7d5e7d0bc21a2beed31141205f\n",
      "\n",
      "1. Title: Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog - year: 2025\n",
      "URL: https://www.semanticscholar.org/paper/bf0257efd52561519fd6d12908de88c94777d872\n",
      "\n",
      "2. Title: On the Partial Identifiability in Reward Learning: Choosing the Best Reward - year: 2025\n",
      "URL: https://www.semanticscholar.org/paper/0475eed79b448112d91f2799a4ec7d273223c087\n",
      "\n",
      "3. Title: Reinforcement Learning From Imperfect Corrective Actions And Proxy Rewards - year: 2024\n",
      "URL: https://www.semanticscholar.org/paper/123d8f72a57e7f956d383de500d5abf32380fd68\n",
      "\n",
      "4. Title: Building Machines that Learn and Think with People - year: 2024\n",
      "URL: https://www.semanticscholar.org/paper/d35f1042defe9a1d69c343ce0237f14d057f48b8\n",
      "\n",
      "5. Title: Value Internalization: Learning and Generalizing from Social Reward - year: 2024\n",
      "URL: https://www.semanticscholar.org/paper/39e82613b490921675efd0004fef68cd15a140e1\n",
      "\n",
      "6. Title: How does Inverse RL Scale to Large State Spaces? A Provably Efficient Approach - year: 2024\n",
      "URL: https://www.semanticscholar.org/paper/e1accf16a3d6bfef9d6fbb729fe41e8244a818f7\n",
      "\n",
      "7. Title: Human-Modeling in Sequential Decision-Making: An Analysis through the Lens of Human-Aware AI - year: 2024\n",
      "URL: https://www.semanticscholar.org/paper/2a4adab65e77cdc5277689139f08e2430aba01b9\n",
      "\n",
      "8. Title: Expectation Alignment: Handling Reward Misspecification in the Presence of Expectation Mismatch - year: 2024\n",
      "URL: https://www.semanticscholar.org/paper/7136ef490cc2e0e032c7c0d95a6926cef400ce34\n",
      "\n",
      "9. Title: Learning Human Preferences Over Robot Behavior as Soft Planning Constraints - year: 2024\n",
      "URL: https://www.semanticscholar.org/paper/14ba38ecdd58366582754507b5d61465d8d6aff9\n",
      "\n",
      "10. Title: Inverse Decision Modeling: Learning Interpretable Representations of Behavior - year: 2023\n",
      "URL: https://www.semanticscholar.org/paper/4d7032df86929584bb7a82e7bdb3a138f12f0719\n",
      "\n",
      "11. Title: Inferring the Goals of Communicating Agents from Actions and Instructions - year: 2023\n",
      "URL: https://www.semanticscholar.org/paper/f709ab376f88a50f0f09508f0b3933b7b0f489f6\n",
      "\n",
      "12. Title: Data Quality in Imitation Learning - year: 2023\n",
      "URL: https://www.semanticscholar.org/paper/a91b821a95ccd417e0f1315247732dd4bcf45991\n",
      "\n",
      "13. Title: Reward Learning With Intractable Normalizing Functions - year: 2023\n",
      "URL: https://www.semanticscholar.org/paper/4d9ad31e9e2c5040cda1adab12e80677dfdc77f2\n",
      "\n",
      "14. Title: Perspectives on the Social Impacts of Reinforcement Learning with Human Feedback - year: 2023\n",
      "URL: https://www.semanticscholar.org/paper/67006554db9732ca23d12ecebfb3e75ea9575c23\n",
      "\n",
      "15. Title: Active Reward Learning from Multiple Teachers - year: 2023\n",
      "URL: https://www.semanticscholar.org/paper/c28af43206867cb5529164e3dd6d9ea8b7cfe7f2\n",
      "\n",
      "16. Title: Benchmarks and Algorithms for Offline Preference-Based Reward Learning - year: 2023\n",
      "URL: https://www.semanticscholar.org/paper/88ae720354fa4883be13e25e1cf92dcd4fed5f5d\n",
      "\n",
      "17. Title: The Effect of Modeling Human Rationality Level on Learning Rewards from Multiple Feedback Types - year: 2022\n",
      "URL: https://www.semanticscholar.org/paper/e920f426eb32e64474b2a1176d97725f875dd82a\n",
      "\n",
      "18. Title: Unified Learning from Demonstrations, Corrections, and Preferences during Physical Human–Robot Interaction - year: 2022\n",
      "URL: https://www.semanticscholar.org/paper/0503229b89eb4611ac4a0d904541c7e2e3bf0ee2\n",
      "\n",
      "19. Title: Humans are not Boltzmann Distributions: Challenges and Opportunities for Modelling Human Feedback and Interaction in Reinforcement Learning - year: 2022\n",
      "URL: https://www.semanticscholar.org/paper/6fb26101dc28d6972abccd0c4e9b6f7be07d86fe\n",
      "\n",
      "20. Title: How to talk so AI will learn: Instructions, descriptions, and autonomy - year: 2022\n",
      "URL: https://www.semanticscholar.org/paper/9c9d9b026a9497e73624912aa9ea905d70e7d470\n",
      "\n",
      "21. Title: Theory of Mind and Preference Learning at the Interface of Cognitive Science, Neuroscience, and AI: A Review - year: 2022\n",
      "URL: https://www.semanticscholar.org/paper/b83eb34b088b31118974f33109ce53a32209d73a\n",
      "\n",
      "22. Title: Invariance in Policy Optimisation and Partial Identifiability in Reward Learning - year: 2022\n",
      "URL: https://www.semanticscholar.org/paper/3ba0be15b292aa4107a4f06da100fd98490c1b39\n",
      "\n",
      "23. Title: Safe Deep RL in 3D Environments using Human Feedback - year: 2022\n",
      "URL: https://www.semanticscholar.org/paper/f354ceedbd0e8ca94e9ff32845805cc92d5475d8\n",
      "\n",
      "24. Title: Learning Submodular Objectives for Team Environmental Monitoring - year: 2021\n",
      "URL: https://www.semanticscholar.org/paper/6946ea2d6565296b2ebf8ebc6e59a6385618f2e1\n",
      "\n",
      "25. Title: Interactive Inverse Reinforcement Learning for Cooperative Games - year: 2021\n",
      "URL: https://www.semanticscholar.org/paper/4aa773b3d8798dc78f1fc4c1a027253bfe9366ad\n",
      "\n",
      "26. Title: Understanding the Relationship between Interactions and Outcomes in Human-in-the-Loop Machine Learning - year: 2021\n",
      "URL: https://www.semanticscholar.org/paper/2f516ddbbf43c69742015a155b28ed65bbdc2880\n",
      "\n",
      "27. Title: Interaction Considerations in Learning from Humans - year: 2021\n",
      "URL: https://www.semanticscholar.org/paper/149b64b24f2ecd2386214b185866cfb583c8d1e7\n",
      "\n",
      "28. Title: Offline Preference-Based Apprenticeship Learning - year: 2021\n",
      "URL: https://www.semanticscholar.org/paper/a1d8cc2fdb7b0f262dc5b057f13bdcaea695842a\n",
      "\n",
      "29. Title: Here’s What I’ve Learned: Asking Questions that Reveal Reward Learning - year: 2021\n",
      "URL: https://www.semanticscholar.org/paper/3995022bf28a4ab773c666e6be300cd5f6c006db\n",
      "\n",
      "30. Title: Learning from an Exploring Demonstrator: Optimal Reward Estimation for Bandits - year: 2021\n",
      "URL: https://www.semanticscholar.org/paper/4b5d7b01bf69daa9cfe76bbf1ed854bac61fa389\n",
      "\n",
      "31. Title: Learning Human Objectives from Sequences of Physical Corrections - year: 2021\n",
      "URL: https://www.semanticscholar.org/paper/1d944b57ca35cf8f3a89841ecee19d3731656c41\n",
      "\n",
      "32. Title: Information Directed Reward Learning for Reinforcement Learning - year: 2021\n",
      "URL: https://www.semanticscholar.org/paper/0707738c08009bc84e0836dcccb608a639a70f87\n",
      "\n",
      "33. Title: REALab: An Embedded Perspective on Tampering - year: 2020\n",
      "URL: https://www.semanticscholar.org/paper/5cde940de9cc7cac8595e790e397ca56b5316202\n",
      "\n",
      "34. Title: I Know What You Meant: Learning Human Objectives by (Under)estimating Their Choice Set - year: 2020\n",
      "URL: https://www.semanticscholar.org/paper/9ea8a61f8fa09244686fc0a9031dc3311ed37c60\n",
      "\n",
      "35. Title: Optimizing Interactive Systems via Data-Driven Objectives - year: 2020\n",
      "URL: https://www.semanticscholar.org/paper/b21dcffbc50ce121d54cb3d3931f543e5789f33c\n",
      "\n",
      "36. Title: Active preference-based Gaussian process regression for reward learning and optimization - year: 2020\n",
      "URL: https://www.semanticscholar.org/paper/061bfda5b75e03dcb15ee4e94e841c2aeeaeccbf\n",
      "\n",
      "37. Title: Optimizing Interactive Systems with Data-Driven Objectives - year: 2018\n",
      "URL: https://www.semanticscholar.org/paper/754ab31270e485c4d2d5b6753ce8223bc8cb39cf\n",
      "\n",
      "38. Title: When Your AIs Deceive You: Challenges with Partial Observability of Human Evaluators in Reward Learning - year: 2024\n",
      "URL: https://www.semanticscholar.org/paper/aae77645ecf5b53be3df128c3919c94bcbd02f28\n",
      "\n",
      "39. Title: Adversarial Imitation Learning with Preferences - year: 2023\n",
      "URL: https://www.semanticscholar.org/paper/74e3b120e375e2fa98bbee0241f6ed3eeec75a3c\n",
      "\n",
      "40. Title: Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining - year: 2023\n",
      "URL: https://www.semanticscholar.org/paper/1123d1ff70881729cd9085147603189efe6d4cc6\n",
      "\n",
      "41. Title: Learning Human Contribution Preferences in Collaborative Human-Robot Tasks - year: 2023\n",
      "URL: https://www.semanticscholar.org/paper/bb6fc9b69cf482083b931df8ec4e5d2388b0a883\n",
      "\n",
      "42. Title: On the Challenges and Practices of Reinforcement Learning from Real Human Feedback - year: 2023\n",
      "URL: https://www.semanticscholar.org/paper/8fa6b111d810c928bbdc966303a7ea57cce57f49\n",
      "\n",
      "43. Title: How to talk so your AI will learn: Instructions, descriptions, and autonomy - year: 2022\n",
      "URL: https://www.semanticscholar.org/paper/5aa85e72597e1cc3b851af007af30124b0ca5b59\n",
      "\n",
      "44. Title: How to talk so your robot will learn: Instructions, descriptions, and pragmatics - year: 2022\n",
      "URL: https://www.semanticscholar.org/paper/8c206a4a0457dfa388898e3bf07d0385d2fdbd5d\n",
      "\n",
      "45. Title: Learning Grounded Pragmatic Communication - year: 2021\n",
      "URL: https://www.semanticscholar.org/paper/80d5684c1e7f5ae34b524ed25dd51f2aa2b613c9\n",
      "\n",
      "46. Title: Interactive Robot Learning: An Overview - year: 2021\n",
      "URL: https://www.semanticscholar.org/paper/ede60a10f12f4f1a966abd7fc034f5187c4d6b08\n",
      "\n",
      "47. Title: Beneﬁts of Assistance over Reward Learning - year: 2020\n",
      "URL: https://www.semanticscholar.org/paper/6a02153b97823b08283b17f964d8d17ddf3c1ccd\n",
      "\n",
      "48. Title: Toward Measuring the Effect of Robot Competency on Human Kinesthetic Feedback in Long-Term Task Learning - year: None\n",
      "URL: https://www.semanticscholar.org/paper/1cbb8b5bba10c3fabf839b687894e23efaa4deaa\n",
      "\n",
      "49. Title: Failure Modes of Learning Reward Models for LLMs and other Sequence Models - year: None\n",
      "URL: https://www.semanticscholar.org/paper/c35fb9c7f10a60488ded4a1bb520e68c9ec957a5\n",
      "\n",
      "50. Title: Scalable Oversight by Accounting for Unreliable Feedback - year: None\n",
      "URL: https://www.semanticscholar.org/paper/58ebc7f497eb8770bdd53ba247710a2ecee88950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, ref in enumerate(subset_refs):\n",
    "    title = ref['citingPaper'].get('title', 'No Title Available')\n",
    "    url = ref['citingPaper'].get('url', 'No URL Available')\n",
    "    print(f\"{i}. Title: {title} - year: {ref['citingPaper'].get('year', 'N/A')}\")\n",
    "    print(f\"URL: {url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84615359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'isInfluential': True,\n",
       "  'contexts': ['Jeon et al. (2020) interpret many of types of feedback as part of an overarching formalism, reward-rational (implicit) choice (RRC), providing a mathematical theory for reward learning that combines different types of feedback.',\n",
       "   'Reward-rational preference orderings over observations, the basis of LEOPARD, are a generalisation of the deterministic reward-rational choice framework (Jeon et al., 2020), but offer several distinct advantages.',\n",
       "   'First, we develop a general mathematical framework, reward-rational partial orderings (RRPO), extending that of deterministic reward-rational choice (RRC, Jeon et al. (2020)).'],\n",
       "  'intents': [],\n",
       "  'citingPaper': {'paperId': '72329bf059a3ae7d5e7d0bc21a2beed31141205f',\n",
       "   'url': 'https://www.semanticscholar.org/paper/72329bf059a3ae7d5e7d0bc21a2beed31141205f',\n",
       "   'title': 'Learning from Preferences and Mixed Demonstrations in General Settings',\n",
       "   'year': 2025,\n",
       "   'authors': [{'authorId': '2376282484', 'name': 'Jason R Brown'},\n",
       "    {'authorId': '2324782232', 'name': 'Carl Henrik Ek'},\n",
       "    {'authorId': '2376265247', 'name': 'Robert D Mullins'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['Related to this, game-theoretic perspective has also been used in dialog modeling (Jeon et al., 2020; Lin et al., 2022).',\n",
       "   '…response generation (He et al., 2017; Jiang et al., 2019; Meta Fundamental AI Research Diplo-macy Team (FAIR) et al., 2022), or optimize for superficial conversation properties using narrowly defined objectives (Khani et al., 2018; Dafoe et al., 2020; Lin et al., 2024; Jeon et al., 2020).'],\n",
       "  'intents': [],\n",
       "  'citingPaper': {'paperId': 'bf0257efd52561519fd6d12908de88c94777d872',\n",
       "   'url': 'https://www.semanticscholar.org/paper/bf0257efd52561519fd6d12908de88c94777d872',\n",
       "   'title': 'Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog',\n",
       "   'year': 2025,\n",
       "   'authors': [{'authorId': '2147166593', 'name': 'Lautaro Estienne'},\n",
       "    {'authorId': '2359257589', 'name': 'Gabriel Ben Zenou'},\n",
       "    {'authorId': '2372259388', 'name': 'Nona Naderi'},\n",
       "    {'authorId': '2288533808', 'name': 'J. Cheung'},\n",
       "    {'authorId': '2274337028', 'name': 'Pablo Piantanida'}]}},\n",
       " {'isInfluential': True,\n",
       "  'contexts': ['Similarly, in trajectory demonstrations (Jeon et al., 2020), there is a single demonstrated trajectory ω .',\n",
       "   'Reward Learning (ReL) is the problem of learning a reward function from data (Jeon et al., 2020).',\n",
       "   'In ReL (Jeon et al., 2020), we aim to learn a target reward r ‹ from a given set of feedback F “ t f i u i , i.e., data, that “leak information” on r ‹ .',\n",
       "   'All ReL papers make the underlying assumption that the feedback received is “reward-rational” (Jeon et al., 2020), in the sense that there exists a reward function r ‹ that drives it (see Assumption 3.1).'],\n",
       "  'intents': [],\n",
       "  'citingPaper': {'paperId': '0475eed79b448112d91f2799a4ec7d273223c087',\n",
       "   'url': 'https://www.semanticscholar.org/paper/0475eed79b448112d91f2799a4ec7d273223c087',\n",
       "   'title': 'On the Partial Identifiability in Reward Learning: Choosing the Best Reward',\n",
       "   'year': 2025,\n",
       "   'authors': [{'authorId': '2215274329', 'name': 'Filippo Lazzati'},\n",
       "    {'authorId': '24717227', 'name': 'Alberto Maria Metelli'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['…advising (Maclin & Shavlik, 1996; Da Silva et al., 2020; Ilhan et al., 2021a), interventions (Peng et al., 2023; Luo et al., 2024), example-states (Eysenbach et al., 2021), or combination of multiple feedback types (Ibarz et al., 2018; Jeon et al., 2020; Yuan et al., 2024; Dong et al., 2024).',\n",
       "   'Indeed, while a precise reward function capturing all aspects of a desired behavior is hard to define for a system designer providing a proxy reward function to roughly express the system designer’s objectives is much easier to achieve (Reddy et al., 2019; Jeon et al., 2020; Luo et al., 2024).'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '123d8f72a57e7f956d383de500d5abf32380fd68',\n",
       "   'url': 'https://www.semanticscholar.org/paper/123d8f72a57e7f956d383de500d5abf32380fd68',\n",
       "   'title': 'Reinforcement Learning From Imperfect Corrective Actions And Proxy Rewards',\n",
       "   'year': 2024,\n",
       "   'authors': [{'authorId': '47653888', 'name': 'Zhaohui Jiang'},\n",
       "    {'authorId': '2325109051', 'name': 'Xuening Feng'},\n",
       "    {'authorId': '2323426226', 'name': 'Paul Weng'},\n",
       "    {'authorId': '2324979272', 'name': 'Yifei Zhu'},\n",
       "    {'authorId': '2157996736', 'name': 'Yan Song'},\n",
       "    {'authorId': '2271834549', 'name': 'Tianze Zhou'},\n",
       "    {'authorId': '1776850', 'name': 'Yujing Hu'},\n",
       "    {'authorId': '80892810', 'name': 'Tangjie Lv'},\n",
       "    {'authorId': '2256482083', 'name': 'Changjie Fan'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['The future of collaborative cognition is bright, but not without risk; continual collaboration and knowledge sharing amongst behavioral scientists, AI practitioners, domain experts, and related disciplines is crucial as we strive to build machines that truly learn and think with people.',\n",
       "   'Rational Speech Acts (RSA) Humans reason about language as an intentional, communicative action to infer a speakers’ underlying goals.'],\n",
       "  'intents': [],\n",
       "  'citingPaper': {'paperId': 'd35f1042defe9a1d69c343ce0237f14d057f48b8',\n",
       "   'url': 'https://www.semanticscholar.org/paper/d35f1042defe9a1d69c343ce0237f14d057f48b8',\n",
       "   'title': 'Building Machines that Learn and Think with People',\n",
       "   'year': 2024,\n",
       "   'authors': [{'authorId': '2055306799', 'name': 'Katherine M. Collins'},\n",
       "    {'authorId': '2226897111', 'name': 'Ilia Sucholutsky'},\n",
       "    {'authorId': '32326200', 'name': 'Umang Bhatt'},\n",
       "    {'authorId': '2070680937', 'name': 'Kartik Chandra'},\n",
       "    {'authorId': '2284592556', 'name': 'Lionel Wong'},\n",
       "    {'authorId': '2315578196', 'name': 'Mina Lee'},\n",
       "    {'authorId': '152592091', 'name': 'Cedegao E. Zhang'},\n",
       "    {'authorId': '120636597', 'name': 'Tan Zhi-Xuan'},\n",
       "    {'authorId': '2263860819', 'name': 'Mark K. Ho'},\n",
       "    {'authorId': '1735083', 'name': 'Vikash K. Mansinghka'},\n",
       "    {'authorId': '2267117825', 'name': 'Adrian Weller'},\n",
       "    {'authorId': '2284592680', 'name': 'Joshua B. Tenenbaum'},\n",
       "    {'authorId': '2247786963', 'name': 'Thomas L. Griffiths'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['Direct forms of feed-back, such as praise, smiles, laughs, punishments, comparison, and correction, and more indirect forms of feedback, such as instruction or demonstration (Jeon, Milli, & Dragan, 2020).',\n",
       "   'Other forms of social feedback, such as observation, demonstrations, language, or corrections, may need their own inferential machinery to distill into an ISR model (Colas et al., 2020; Jeon et al., 2020; Kleiman-Weiner et al., 2020).'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '39e82613b490921675efd0004fef68cd15a140e1',\n",
       "   'url': 'https://www.semanticscholar.org/paper/39e82613b490921675efd0004fef68cd15a140e1',\n",
       "   'title': 'Value Internalization: Learning and Generalizing from Social Reward',\n",
       "   'year': 2024,\n",
       "   'authors': [{'authorId': '2312326772', 'name': 'Frieda Rong'},\n",
       "    {'authorId': '2312325194', 'name': 'Max Kleiman-Weiner'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['With regards to the more general field of Reward Learning (ReL), we mention [19], which introduces a framework that formalizes the constraints imposed by various kinds of human feedback (like demonstrations or preferences [65]).',\n",
       "   'In Reward Learning (ReL) works [19], demonstrations of optimal behavior are combined with other kinds of expert feedback, like comparisons [65].'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': 'e1accf16a3d6bfef9d6fbb729fe41e8244a818f7',\n",
       "   'url': 'https://www.semanticscholar.org/paper/e1accf16a3d6bfef9d6fbb729fe41e8244a818f7',\n",
       "   'title': 'How does Inverse RL Scale to Large State Spaces? A Provably Efficient Approach',\n",
       "   'year': 2024,\n",
       "   'authors': [{'authorId': '2215274329', 'name': 'Filippo Lazzati'},\n",
       "    {'authorId': '51004138', 'name': 'Mirco Mutti'},\n",
       "    {'authorId': '24717227', 'name': 'Alberto Maria Metelli'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['The most widely used model, i.e., noisy rational model [16], is known to be insufficient in many cases.',\n",
       "   'Generally, humans are widely accepted to be bounded rational agents [16], even though they are not always modeled as such.'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': '2a4adab65e77cdc5277689139f08e2430aba01b9',\n",
       "   'url': 'https://www.semanticscholar.org/paper/2a4adab65e77cdc5277689139f08e2430aba01b9',\n",
       "   'title': 'Human-Modeling in Sequential Decision-Making: An Analysis through the Lens of Human-Aware AI',\n",
       "   'year': 2024,\n",
       "   'authors': [{'authorId': '72038684', 'name': 'Silvia Tulli'},\n",
       "    {'authorId': '38385833', 'name': 'S. Vasileiou'},\n",
       "    {'authorId': '2400282', 'name': 'S. Sreedharan'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['Along the same lines, we could leverage psychologically feasible models as a stand-in for P H to further improve the accuracy of our estimates (possible candidates include noisy-rational model [Jeon et al., 2020]).',\n",
       "   'A popular model for approximating human decision-making is to use the noisy rational model Jeon et al. [2020].'],\n",
       "  'intents': ['methodology'],\n",
       "  'citingPaper': {'paperId': '7136ef490cc2e0e032c7c0d95a6926cef400ce34',\n",
       "   'url': 'https://www.semanticscholar.org/paper/7136ef490cc2e0e032c7c0d95a6926cef400ce34',\n",
       "   'title': 'Expectation Alignment: Handling Reward Misspecification in the Presence of Expectation Mismatch',\n",
       "   'year': 2024,\n",
       "   'authors': [{'authorId': '2203917617', 'name': 'Malek Mechergui'},\n",
       "    {'authorId': '2400282', 'name': 'S. Sreedharan'}]}},\n",
       " {'isInfluential': True,\n",
       "  'contexts': ['We consider explicit feedback provided through comparison queries because this interaction modality is popular in HRI (e.g., [9], [10], [11], [12]).',\n",
       "   'Simulating Noisy User Choices: We simulate human choices in binary queries using a bounded rationality model [47], [10].',\n",
       "   'Robot behavior can be learned or adapted using different types of interaction modalities [10], which can be explicit or implicit.',\n",
       "   'Such a bounded rationality model for human decision-making has been widely used in the preference learning literature to simulate human choices [48], [5], [10].',\n",
       "   'Additionally, we focus on learning preferences using binary queries in which users indicate their preference among two trajectories of execution of robot behavior because this interaction modality is popular in the HRI literature (e.g., [9], [10], [11], [12]).',\n",
       "   'Explicit interaction modalities include demonstrations of desired robot behavior [1] and comparison queries [9], [10], [11] whereas implicit communication occurs via social cues like gaze [25], [26] or facial reactions [27], [28].'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': '14ba38ecdd58366582754507b5d61465d8d6aff9',\n",
       "   'url': 'https://www.semanticscholar.org/paper/14ba38ecdd58366582754507b5d61465d8d6aff9',\n",
       "   'title': 'Learning Human Preferences Over Robot Behavior as Soft Planning Constraints',\n",
       "   'year': 2024,\n",
       "   'authors': [{'authorId': '2294173414', 'name': 'Austin Narcomey'},\n",
       "    {'authorId': '39282796', 'name': 'Nathan Tsoi'},\n",
       "    {'authorId': '2265650981', 'name': 'Ruta Desai'},\n",
       "    {'authorId': '144168587', 'name': 'Marynel Vázquez'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['To corresponds exactly to the proposed method of [22] itself, which is similar to IOHMM-Based BC-IL except parameters are trained jointly.',\n",
       "   '001 until convergence (non-improvement for 100 consecutive iterations).'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': '4d7032df86929584bb7a82e7bdb3a138f12f0719',\n",
       "   'url': 'https://www.semanticscholar.org/paper/4d7032df86929584bb7a82e7bdb3a138f12f0719',\n",
       "   'title': 'Inverse Decision Modeling: Learning Interpretable Representations of Behavior',\n",
       "   'year': 2023,\n",
       "   'authors': [{'authorId': '123723354', 'name': 'Daniel Jarrett'},\n",
       "    {'authorId': '83246796', 'name': 'Alihan Hüyük'},\n",
       "    {'authorId': '1729969', 'name': 'M. Schaar'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['Most closely re-lated is reward-rational implicit choice (Jeon et al., 2020), a framework for multimodal Bayesian reward learning from multiple types of human feedback, including demonstrations and language.',\n",
       "   'Unlike related work that explores how the assistant should infer the principal’s goal (Hadﬁeld-Menell et al., 2016; Jeon et al., 2020; Squire et al., 2015), our task is to infer the team’s goal given their actions and communicated instructions, producing a distribution over goals (Figure 1(c)).'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': 'f709ab376f88a50f0f09508f0b3933b7b0f489f6',\n",
       "   'url': 'https://www.semanticscholar.org/paper/f709ab376f88a50f0f09508f0b3933b7b0f489f6',\n",
       "   'title': 'Inferring the Goals of Communicating Agents from Actions and Instructions',\n",
       "   'year': 2023,\n",
       "   'authors': [{'authorId': '2142478242', 'name': 'Lance Ying'},\n",
       "    {'authorId': '120636597', 'name': 'Tan Zhi-Xuan'},\n",
       "    {'authorId': '1735083', 'name': 'Vikash K. Mansinghka'},\n",
       "    {'authorId': '1763295', 'name': 'J. Tenenbaum'}]}},\n",
       " {'isInfluential': True,\n",
       "  'contexts': ['Active learning methods have also been developed to guide data collection towards more informative samples [15, 6, 41, 29].',\n",
       "   'In prior data-centric methods, the goal is usually to maximize state diversity through shared control [45, 44, 16, 27, 31, 35, 46], noise injection [33], or active queries [15, 6, 41, 29].',\n",
       "   'Many prior works seek to improve the state coverage of the dataset, using some metric similar to Definition 1 [6, 29].',\n",
       "   '[29] Hong Jun Jeon, Smitha Milli, and Anca D Dragan.'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': 'a91b821a95ccd417e0f1315247732dd4bcf45991',\n",
       "   'url': 'https://www.semanticscholar.org/paper/a91b821a95ccd417e0f1315247732dd4bcf45991',\n",
       "   'title': 'Data Quality in Imitation Learning',\n",
       "   'year': 2023,\n",
       "   'authors': [{'authorId': '69879999', 'name': 'Suneel Belkhale'},\n",
       "    {'authorId': '7332443', 'name': 'Yuchen Cui'},\n",
       "    {'authorId': '1779671', 'name': 'Dorsa Sadigh'}]}},\n",
       " {'isInfluential': True,\n",
       "  'contexts': [', state-action pairs [2] or trajectories [1] provided by the human), the robot arm infers a posterior over the space of possible rewards.',\n",
       "   'For example, in related works [1], [16] the reward function is often a linear combination of features such that r(s, θ) = θ · φ(s).',\n",
       "   'Related work formalizes this as Bayesian inference [1], [2], [3].',\n",
       "   'For instance, consider a human that repeatedly demonstrates a task to the robot: each demonstration depends on their reward θ, but the human does not reason over ξi when selecting ξj [1], [3], [6].',\n",
       "   'State-of-the-art research often tackles this reward learning problem using Bayesian inference [1], [2], [3].'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': '4d9ad31e9e2c5040cda1adab12e80677dfdc77f2',\n",
       "   'url': 'https://www.semanticscholar.org/paper/4d9ad31e9e2c5040cda1adab12e80677dfdc77f2',\n",
       "   'title': 'Reward Learning With Intractable Normalizing Functions',\n",
       "   'year': 2023,\n",
       "   'authors': [{'authorId': '2189304741', 'name': 'Joshua Hoegerman'},\n",
       "    {'authorId': '2426559', 'name': 'Dylan P. Losey'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['How will agents extract and make sense of various sources of information? Choose between multiple forms of feedback (for example: comparisons, demonstrations, corrections, improvement, proxy rewards, punishments, credit assignment, linguistic instructions [38])? Distinguish purposeful from meaningless feedbacks? These considerations will become increasingly important as RLHF advances.',\n",
       "   'There will inevitably be new ways invented for humans to meaningfully provide feedback to robots and AI agents, as well as new insights as to how human behavior inherently and subtly reveals information signals at any given point [38].'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '67006554db9732ca23d12ecebfb3e75ea9575c23',\n",
       "   'url': 'https://www.semanticscholar.org/paper/67006554db9732ca23d12ecebfb3e75ea9575c23',\n",
       "   'title': 'Perspectives on the Social Impacts of Reinforcement Learning with Human Feedback',\n",
       "   'year': 2023,\n",
       "   'authors': [{'authorId': '2328852241',\n",
       "     'name': 'Gabrielle Kaili-May Liu'}]}},\n",
       " {'isInfluential': True,\n",
       "  'contexts': ['We will follow recent work on learning from human preferences [7, 9, 12, 14] and model human teachers as Boltzmann-rational, making choices according to a wellknown probability model specified later in the paper.',\n",
       "   'Reward learning techniques enable AI systems to learn their objectives by observing and interacting with humans instead of requiring their designers to specify these objectives manually [9].',\n",
       "   'Modeling Human Rationality Human teachers can be represented as Boltzmann-rational agents following a large body of existing work on reward learning [7, 9, 12, 14, 30, 31, 32, 33, 34].',\n",
       "   'Later work models human behavior as pedagogic [24], systematically biased [28], and noisily or Boltzmann-rational [9, 12].'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': 'c28af43206867cb5529164e3dd6d9ea8b7cfe7f2',\n",
       "   'url': 'https://www.semanticscholar.org/paper/c28af43206867cb5529164e3dd6d9ea8b7cfe7f2',\n",
       "   'title': 'Active Reward Learning from Multiple Teachers',\n",
       "   'year': 2023,\n",
       "   'authors': [{'authorId': '2158184892', 'name': 'Peter Barnett'},\n",
       "    {'authorId': '50632861', 'name': 'Rachel Freedman'},\n",
       "    {'authorId': '2389770', 'name': 'Justin Svegliato'},\n",
       "    {'authorId': '145107462', 'name': 'Stuart J. Russell'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': [', 2018; Arora & Doshi, 2021), preference queries—where a user is asked to compare two trajectory segments—have been shown to more accurately identify the reward (Jeon et al., 2020) while reducing the burden on the user to generate near-optimal actions (Wirth et al.',\n",
       "   '…2018; Arora & Doshi, 2021), preference queries—where a user is asked to compare two trajectory segments—have been shown to more accurately identify the reward (Jeon et al., 2020) while reducing the burden on the user to generate near-optimal actions (Wirth et al., 2017; Christiano et al.,\\n2017).'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '88ae720354fa4883be13e25e1cf92dcd4fed5f5d',\n",
       "   'url': 'https://www.semanticscholar.org/paper/88ae720354fa4883be13e25e1cf92dcd4fed5f5d',\n",
       "   'title': 'Benchmarks and Algorithms for Offline Preference-Based Reward Learning',\n",
       "   'year': 2023,\n",
       "   'authors': [{'authorId': '2119996488', 'name': 'Daniel Shin'},\n",
       "    {'authorId': '2745001', 'name': 'A. Dragan'},\n",
       "    {'authorId': '2115445928', 'name': 'Daniel S. Brown'}]}},\n",
       " {'isInfluential': True,\n",
       "  'contexts': ['Recently, it was shown that all of these can be interpreted as noisy-rational (Boltzmann) choices (Jeon, Milli, and Dragan 2020), opening the door to learning from all of these feedback types in combination, and even enabling robots to actively select feedback types.',\n",
       "   'Following prior work (Jeon, Milli, and Dragan 2020), we interpret these varying forms of feedback as a rewardrational choice over a (potentially implicit) choice set C.',\n",
       "   'It turned out all of these can be interpreted as noisy-rational (Boltzmann) choices [26], opening the door to learning from all of these in combination, and even enabling robots to actively select what feedback type to ask for.',\n",
       "   '[26] for a discussion of how many feedback types can be formalized similarly.',\n",
       "   'The likelihood that the human prefers trajectory A over B\\n2Jeon et al. (Jeon, Milli, and Dragan 2020) model human demonstrations as choices over all possible trajectories; however, with stochastic dynamics, human actions are conditioned on observed state transitions and the human cannot pre-select a specific trajectory.\\nis given by the Bradley-Luce-Shepherd rule (Bradley and Terry 1952; Christiano et al. 2017):\\nP (ξA | r, β) = exp (β · r(ξA))\\nexp (β · r(ξA)) + exp (β · r(ξB)) (2)\\nE-stops represent the intervention of a human telling the robot to stop rather than continue its trajectory.',\n",
       "   'The likelihood that the human prefers trajectory A over B\\n2Jeon et al. (Jeon, Milli, and Dragan 2020) model human demonstrations as choices over all possible trajectories; however, with stochastic dynamics, human actions are conditioned on observed state transitions and the human cannot pre-select…',\n",
       "   'Boltzmann-rationality’s ability to unify different feedback types is useful, but the model comes with this one parameter—β, which begs the question: what should we set that to? Prior work often either omits β (implicitly setting it to 1) [17, 13, 25] or sets it to a fixed, often heuristic, value across all feedback types [37, 26, 39, 4].',\n",
       "   'We refer the reader to Jeon et al. (Jeon, Milli, and Dragan 2020) for a discussion of how many other feedback types can be formalized similarly.',\n",
       "   'Following prior work [26], we interpret these varying forms of feedback as a reward-rational choice over a (potentially implicit) choice set C.',\n",
       "   '[26] model demos as choices over all possible trajectories; however, with stochastic dynamics, human actions are conditioned on observed state transitions and the human cannot pre-select a specific trajectory.',\n",
       "   '…work often either omits β (implicitly setting it to 1) (Finn, Levine, and Abbeel 2016; Christiano et al. 2017; Ibarz et al. 2018) or sets it to a fixed, often heuristic, value across all feedback types (Ramachandran and Amir 2007; Shah et al. 2019; Bıyık et al. 2020; Jeon, Milli, and Dragan 2020).'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': 'e920f426eb32e64474b2a1176d97725f875dd82a',\n",
       "   'url': 'https://www.semanticscholar.org/paper/e920f426eb32e64474b2a1176d97725f875dd82a',\n",
       "   'title': 'The Effect of Modeling Human Rationality Level on Learning Rewards from Multiple Feedback Types',\n",
       "   'year': 2022,\n",
       "   'authors': [{'authorId': '1825768411', 'name': 'Gaurav R. Ghosal'},\n",
       "    {'authorId': '2075365014', 'name': 'M. Zurek'},\n",
       "    {'authorId': '2115445928', 'name': 'Daniel S. Brown'},\n",
       "    {'authorId': '2745001', 'name': 'A. Dragan'}]}},\n",
       " {'isInfluential': True,\n",
       "  'contexts': ['Robots should leverage this interaction to learn from the human and improve their own behavior.',\n",
       "   'Here 𝑠 ∈ S is the state of the robot and 𝑎 ∈ A denotes the robot action.',\n",
       "   'On the one hand, methods like [4, 24] require prior knowledge about the human’s reward function — as we will show in our analysis and experiments, these methods fall short when the human wants to teach the robot a new or unexpected task.',\n",
       "   'To teach the robot the human exploits physical interaction: the human kinesthetically guides the robot through the process of attaching a chair leg, modifies specific sections of the robot’s trajectory, and ranks the robot’s autonomous behavior across repeated interactions.',\n",
       "   'We add this human feedback to the dataset of demonstrations D for the first interaction (i.e. if 𝑖 = 1), and to the dataset corrections C , or preferences Q for all other interactions ( 𝑖 > 1).',\n",
       "   'But to learn from multiple sources of human feedback the robot must either (a) have prior information about the tasks the human has in mind [4, 24] or (b) apply reinforcement learning to identify the optimal trajectory [7, 9, 10, 22, 49].',\n",
       "   'Most relevant to our research are [24] and [4], where the authors unite different types of human feedback to learn a single reward model.'],\n",
       "  'intents': [],\n",
       "  'citingPaper': {'paperId': '0503229b89eb4611ac4a0d904541c7e2e3bf0ee2',\n",
       "   'url': 'https://www.semanticscholar.org/paper/0503229b89eb4611ac4a0d904541c7e2e3bf0ee2',\n",
       "   'title': 'Unified Learning from Demonstrations, Corrections, and Preferences during Physical Human–Robot Interaction',\n",
       "   'year': 2022,\n",
       "   'authors': [{'authorId': '1753601436', 'name': 'Shaunak A. Mehta'},\n",
       "    {'authorId': '2426559', 'name': 'Dylan P. Losey'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['For a more thorough review of possible types of human-AI interaction in the context of RL, we refer to recent reviews [15, 19].',\n",
       "   'One of the most common ways to model noisy feedback is using a Boltzmann distribution [15].',\n",
       "   'Unfortunately, existing approaches to learning from human feedback rely on simple human models, such as Boltzmann distributions [15].'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': '6fb26101dc28d6972abccd0c4e9b6f7be07d86fe',\n",
       "   'url': 'https://www.semanticscholar.org/paper/6fb26101dc28d6972abccd0c4e9b6f7be07d86fe',\n",
       "   'title': 'Humans are not Boltzmann Distributions: Challenges and Opportunities for Modelling Human Feedback and Interaction in Reinforcement Learning',\n",
       "   'year': 2022,\n",
       "   'authors': [{'authorId': '4829248', 'name': 'David Lindner'},\n",
       "    {'authorId': '1401917601', 'name': 'Mennatallah El-Assady'}]}},\n",
       " {'isInfluential': True,\n",
       "  'contexts': ['If the speaker’s horizon H is known, we can write a L 1 listener as: Given an instruction, L 1 infers the reward weights that would make such an instruction optimal [13, 14]; given a description, L 1 can recover information about features that were not mentioned [24, 25].',\n",
       "   'We formalized the challenge of using language to teach an agent to act on our behalf [13, 14, 43, 44, 84] in a bandit setting, introducing the notion of a horizon to reﬂect the agent’s degree of autonomy.',\n",
       "   'This paradigm assumes instructions reﬂect underlying preferences and uses them to infer this latent reward function [13, 14].',\n",
       "   'The horizon quantiﬁes notions of autonomy described in previous work [13, 14, 25].',\n",
       "   'Instructions map to speciﬁc actions or trajectories [7, 13]; in our work, “instruction” utterances correspond to the nine actions (Fig 1B).',\n",
       "   'When the state is known, we deﬁne the present utility of an utterance as the expected reward from using the updated policy to choose an action: However, we are interested in settings where agents may need to act autonomously [13, 24, 25].',\n",
       "   'When the desired behavior is known (but the reward function is not), inverse reinforcement learning [IRL, 13, 14, 27, 28, 42, 45, 46] can be used to infer an expert’s reward function from their actions.',\n",
       "   'However, if autonomy is desired [1–3, 13, 14, 38, 43, 44], agents should be equipped to understand descriptions of the world or our preferences [21–25].'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': '9c9d9b026a9497e73624912aa9ea905d70e7d470',\n",
       "   'url': 'https://www.semanticscholar.org/paper/9c9d9b026a9497e73624912aa9ea905d70e7d470',\n",
       "   'title': 'How to talk so AI will learn: Instructions, descriptions, and autonomy',\n",
       "   'year': 2022,\n",
       "   'authors': [{'authorId': '1976174397', 'name': 'T. Sumers'},\n",
       "    {'authorId': '8932668', 'name': 'Robert D. Hawkins'},\n",
       "    {'authorId': '2543534', 'name': 'Mark K. Ho'},\n",
       "    {'authorId': '1799860', 'name': 'T. Griffiths'},\n",
       "    {'authorId': '1397904824', 'name': 'Dylan Hadfield-Menell'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['…shows that there has been signiﬁcant progress on the theoretical side, especially in preference ToM, e.g., combining diﬀerent information sources about human preferences (Hong et al., 2020) and integrating uncertainty (Hadﬁeld-Menell et al., 2017b) and human interaction (Shah et al., 2021a).',\n",
       "   'Hong et al. (2020) provide a simple formalism which can unify previously-proposed algorithms for learning preferences from diﬀerent information sources in a Bayesian setting, by inferring a distribution over the possible rewards.',\n",
       "   'The latter can be good speciﬁcations for scenarios from the training distribution, but not necessarily for novel situations which might occur during deployment [see Hong et al. (2020) for more examples].'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': 'b83eb34b088b31118974f33109ce53a32209d73a',\n",
       "   'url': 'https://www.semanticscholar.org/paper/b83eb34b088b31118974f33109ce53a32209d73a',\n",
       "   'title': 'Theory of Mind and Preference Learning at the Interface of Cognitive Science, Neuroscience, and AI: A Review',\n",
       "   'year': 2022,\n",
       "   'authors': [{'authorId': '1435986650', 'name': 'C. Langley'},\n",
       "    {'authorId': '3391593', 'name': 'B. Cirstea'},\n",
       "    {'authorId': '101980376', 'name': 'F. Cuzzolin'},\n",
       "    {'authorId': '144766323', 'name': 'B. Sahakian'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['Reward learning methods have also been proposed for many other data sources (Jeon et al., 2020).',\n",
       "   ', 2020), or other multimodal data sources (Tung et al., 2018; Krasheninnikov et al., 2019; Jeon et al., 2020).',\n",
       "   'There has been recent interest in the prospect of combining information from multiple different sources for the purpose of reward learning (Krasheninnikov et al., 2019; Jeon et al., 2020).'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '3ba0be15b292aa4107a4f06da100fd98490c1b39',\n",
       "   'url': 'https://www.semanticscholar.org/paper/3ba0be15b292aa4107a4f06da100fd98490c1b39',\n",
       "   'title': 'Invariance in Policy Optimisation and Partial Identifiability in Reward Learning',\n",
       "   'year': 2022,\n",
       "   'authors': [{'authorId': '147156275', 'name': 'Joar Skalse'},\n",
       "    {'authorId': '2282534862', 'name': 'Matthew Farrugia-Roberts'},\n",
       "    {'authorId': '145107462', 'name': 'Stuart J. Russell'},\n",
       "    {'authorId': '144938187', 'name': 'A. Abate'},\n",
       "    {'authorId': '34594377', 'name': 'Adam Gleave'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': [', 2017), and combinations thereof (Ibarz et al., 2018; Stiennon et al., 2020; Jeon et al., 2020).',\n",
       "   '…include real-time scalar rewards (Knox & Stone, 2009; Warnell et al., 2018), goal states (Bahdanau et al., 2019), trajectory demonstrations (Finn et al., 2016), trajectory comparisons (Christiano et al., 2017), and combinations thereof (Ibarz et al., 2018; Stiennon et al., 2020; Jeon et al., 2020).'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': 'f354ceedbd0e8ca94e9ff32845805cc92d5475d8',\n",
       "   'url': 'https://www.semanticscholar.org/paper/f354ceedbd0e8ca94e9ff32845805cc92d5475d8',\n",
       "   'title': 'Safe Deep RL in 3D Environments using Human Feedback',\n",
       "   'year': 2022,\n",
       "   'authors': [{'authorId': '3183032', 'name': 'Matthew Rahtz'},\n",
       "    {'authorId': '144711236', 'name': 'Vikrant Varma'},\n",
       "    {'authorId': '2117776492', 'name': 'Ramana Kumar'},\n",
       "    {'authorId': '40947466', 'name': 'Zachary Kenton'},\n",
       "    {'authorId': '34313265', 'name': 'S. Legg'},\n",
       "    {'authorId': '2990741', 'name': 'Jan Leike'}]}},\n",
       " {'isInfluential': True,\n",
       "  'contexts': ['Since classical approaches such as learning from demonstrations are not always suitable, alternative modes of interaction including corrections, proxy rewards, critique, and choice have been developed [6].',\n",
       "   'Similar approaches are commonly used in reward learning problems [6], [7], [9]–[11], where basis functions are usually referred to as features .',\n",
       "   'This is known as reward learning , where the robot presents the user with one or multiple possible solutions to it’s task and then obtains feedback in the form of corrections, choice, labels, or others [6].',\n",
       "   'To reduce the complexity and thus enable a broader range of users to deploy autonomous robots, researchers have studied different frameworks for reward learning [6]–[17].'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '6946ea2d6565296b2ebf8ebc6e59a6385618f2e1',\n",
       "   'url': 'https://www.semanticscholar.org/paper/6946ea2d6565296b2ebf8ebc6e59a6385618f2e1',\n",
       "   'title': 'Learning Submodular Objectives for Team Environmental Monitoring',\n",
       "   'year': 2021,\n",
       "   'authors': [{'authorId': '51314515', 'name': 'Nils Wilde'},\n",
       "    {'authorId': '31110463', 'name': 'Armin Sadeghi'},\n",
       "    {'authorId': '2157788328', 'name': 'Stephen L. Smith'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['A common decision-model for suboptimal human behaviour in IRL (Jeon et al., 2020), economics (Luce, 1959), and cognitive science (Baker et al.',\n",
       "   'A common decision-model for suboptimal human behaviour in IRL (Jeon et al., 2020), economics (Luce, 1959), and cognitive science (Baker et al., 2009) are Boltzmann-rational policies for which the probability of choosing an action is exponentially dependent on its expected value:\\nπ2(b | s, π1) ∝ exp…'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '4aa773b3d8798dc78f1fc4c1a027253bfe9366ad',\n",
       "   'url': 'https://www.semanticscholar.org/paper/4aa773b3d8798dc78f1fc4c1a027253bfe9366ad',\n",
       "   'title': 'Interactive Inverse Reinforcement Learning for Cooperative Games',\n",
       "   'year': 2021,\n",
       "   'authors': [{'authorId': '2090817723', 'name': 'Thomas Kleine Buening'},\n",
       "    {'authorId': '3016153', 'name': 'Anne-Marie George'},\n",
       "    {'authorId': '1720480', 'name': 'Christos Dimitrakakis'}]}},\n",
       " {'isInfluential': True,\n",
       "  'contexts': ['Jeon et al. [2020] proposed a framework unifying different interaction types in the reward learning literature and compared how different interaction types inﬂuence learning of a reward function assuming optimal inputs.',\n",
       "   'Choice Space: C i ( q ) Once a query has been selected, the process for expanding a query into a set of possible explicit and implicit choices available to the teacher is also interaction-speciﬁc [Jeon et al. , 2020; Koppol et al. , 2021].',\n",
       "   'Recent work of Jeon et al. [2020] proposes a formulation of information gain for ﬁnding the best feedback type for reward learning, assuming optimal feedback.',\n",
       "   'In an information gain context, this implication can be represented as the conditional entropy over the model’s parameters given the feedback that the teacher did and did not provide [Jeon et al. , 2020].',\n",
       "   'First, the interaction type directly determines the form of label that is collected in the training data, as well as the training implications of that label (e.g., how that label should be converted into a reward update [Jeon et al. , 2020]).',\n",
       "   'The work of Jeon et al. [2020] proposes a way to optimize for interaction types for reward learning from the information gain perspective but does not take human performance into account.',\n",
       "   '…makers [Arkes and Ayton, 1999; Hewig et al. , 2011], many existing meth-ods have been assuming rational or noisily rational human teachers and the same human teacher model has been employed by various algorithms that learn from different interaction types [Sadigh et al. , 2017; Jeon et al. , 2020].'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': '2f516ddbbf43c69742015a155b28ed65bbdc2880',\n",
       "   'url': 'https://www.semanticscholar.org/paper/2f516ddbbf43c69742015a155b28ed65bbdc2880',\n",
       "   'title': 'Understanding the Relationship between Interactions and Outcomes in Human-in-the-Loop Machine Learning',\n",
       "   'year': 2021,\n",
       "   'authors': [{'authorId': '7332443', 'name': 'Yuchen Cui'},\n",
       "    {'authorId': '2095700630', 'name': 'Pallavi Koppol'},\n",
       "    {'authorId': '1882027', 'name': 'H. Admoni'},\n",
       "    {'authorId': '1719955', 'name': 'R. Simmons'},\n",
       "    {'authorId': '1792714', 'name': 'Aaron Steinfeld'},\n",
       "    {'authorId': '33965230', 'name': 'Tesca Fitzgerald'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['Credit-assignment interactions, such as the one posed in [Jeon et al. , 2020], can be construed as a form of critique where the user is limited to identifying only one good region.',\n",
       "   'Different interaction types can be used in combination to accelerate learning [Palan et al. , 2019], better leverage people as teachers [Bullard et al. , 2018], and differ in the amount of implicit information they encode [ Jeon et al. , 2020].',\n",
       "   '…the rich learning interactions people use has involved combining multiple interaction types [Bullard et al. , 2018] to better leverage human teachers, leveraging trade-offs between interactions [Palan et al. , 2019], and exploring explicit and implicit information transfer [Jeon et al. , 2020].'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': '149b64b24f2ecd2386214b185866cfb583c8d1e7',\n",
       "   'url': 'https://www.semanticscholar.org/paper/149b64b24f2ecd2386214b185866cfb583c8d1e7',\n",
       "   'title': 'Interaction Considerations in Learning from Humans',\n",
       "   'year': 2021,\n",
       "   'authors': [{'authorId': '2095700630', 'name': 'Pallavi Koppol'},\n",
       "    {'authorId': '1882027', 'name': 'H. Admoni'},\n",
       "    {'authorId': '2054613750', 'name': 'Reid G. Simmons'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['…2018; Arora & Doshi, 2021), preference queries—where a user is asked to compare two trajectory segments—have been shown to more accurately identify the reward (Jeon et al., 2020) while reducing the burden on the user to generate near-optimal actions (Wirth et al., 2017; Christiano et al., 2017).',\n",
       "   ', 2018; Arora & Doshi, 2021), preference queries—where a user is asked to compare two trajectory segments—have been shown to more accurately identify the reward (Jeon et al., 2020) while reducing the burden on the user to generate near-optimal actions (Wirth et al.'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': 'a1d8cc2fdb7b0f262dc5b057f13bdcaea695842a',\n",
       "   'url': 'https://www.semanticscholar.org/paper/a1d8cc2fdb7b0f262dc5b057f13bdcaea695842a',\n",
       "   'title': 'Offline Preference-Based Apprenticeship Learning',\n",
       "   'year': 2021,\n",
       "   'authors': [{'authorId': '2119996488', 'name': 'Daniel Shin'},\n",
       "    {'authorId': '47627548', 'name': 'Daniel S. Brown'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['Similar to prior research, within our simulations and user studies we speciically consider pairwise comparisons where the robot only shows two options [9, 10, 14, 23, 25, 43, 50, 52].',\n",
       "   'Our choice of human model is consistent with prior work on inverse reinforcement learning [25, 36, 53] and cognitive psychology [27, 28, 32].',\n",
       "   'These inputs can take many diferent forms [25], from physical demonstrations to verbal commands to pairwise comparisons.'],\n",
       "  'intents': ['background', 'result'],\n",
       "  'citingPaper': {'paperId': '3995022bf28a4ab773c666e6be300cd5f6c006db',\n",
       "   'url': 'https://www.semanticscholar.org/paper/3995022bf28a4ab773c666e6be300cd5f6c006db',\n",
       "   'title': 'Here’s What I’ve Learned: Asking Questions that Reveal Reward Learning',\n",
       "   'year': 2021,\n",
       "   'authors': [{'authorId': '51171307', 'name': 'Soheil Habibian'},\n",
       "    {'authorId': '2008860791', 'name': 'Ananth Jonnavittula'},\n",
       "    {'authorId': '2426559', 'name': 'Dylan P. Losey'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['…has explored easier settings that have the scope to avoid reward identiﬁability issues in IRL by assuming either additional structure on the reward or access to side-information (Amin et al., 2017; Gershman, 2016; Geng et al., 2020; Ballard and McClure, 2019; Jeon et al., 2020; Fu et al., 2017).',\n",
       "   'Recent work has explored easier settings that have the scope to avoid reward identifiability issues in IRL by assuming either additional structure on the reward or access to side-information (Amin et al., 2017; Gershman, 2016; Geng et al., 2020; Ballard and McClure, 2019; Jeon et al., 2020; Fu et al., 2017).'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '4b5d7b01bf69daa9cfe76bbf1ed854bac61fa389',\n",
       "   'url': 'https://www.semanticscholar.org/paper/4b5d7b01bf69daa9cfe76bbf1ed854bac61fa389',\n",
       "   'title': 'Learning from an Exploring Demonstrator: Optimal Reward Estimation for Bandits',\n",
       "   'year': 2021,\n",
       "   'authors': [{'authorId': '4328852', 'name': 'Wenshuo Guo'},\n",
       "    {'authorId': '6565766', 'name': 'Kumar Krishna Agrawal'},\n",
       "    {'authorId': '1954250', 'name': 'Aditya Grover'},\n",
       "    {'authorId': '145192874', 'name': 'Vidya Muthukumar'},\n",
       "    {'authorId': '3043393', 'name': 'A. Pananjady'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['ment learning [7], [23], [25], [28], we model humans as noisily rational agents whose corrections maximize accumulated evidence for their preferred θ:',\n",
       "   'Similar to prior work [7], [23]–[25], we capture this objective through our reward function: R(ξ; θ) = θ ·Φ(ξ).'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '1d944b57ca35cf8f3a89841ecee19d3731656c41',\n",
       "   'url': 'https://www.semanticscholar.org/paper/1d944b57ca35cf8f3a89841ecee19d3731656c41',\n",
       "   'title': 'Learning Human Objectives from Sequences of Physical Corrections',\n",
       "   'year': 2021,\n",
       "   'authors': [{'authorId': '70623401', 'name': 'Mengxi Li'},\n",
       "    {'authorId': '2061747814', 'name': 'Alper Canberk'},\n",
       "    {'authorId': '2426559', 'name': 'Dylan P. Losey'},\n",
       "    {'authorId': '1779671', 'name': 'Dorsa Sadigh'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['Reward models can also be learned from comparisons of two or more different behaviors (Wirth et al., 2017), or other kinds of feedback (Jeon et al., 2020).',\n",
       "   ', 2017), or other kinds of feedback (Jeon et al., 2020).'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '0707738c08009bc84e0836dcccb608a639a70f87',\n",
       "   'url': 'https://www.semanticscholar.org/paper/0707738c08009bc84e0836dcccb608a639a70f87',\n",
       "   'title': 'Information Directed Reward Learning for Reinforcement Learning',\n",
       "   'year': 2021,\n",
       "   'authors': [{'authorId': '4829248', 'name': 'David Lindner'},\n",
       "    {'authorId': '3422558', 'name': 'M. Turchetta'},\n",
       "    {'authorId': '3302876', 'name': 'Sebastian Tschiatschek'},\n",
       "    {'authorId': '2474449', 'name': 'K. Ciosek'},\n",
       "    {'authorId': '153243248', 'name': 'A. Krause'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['Jeon et al. [2020] also highlight the importance of viewing feedback type as a choice of the algorithm designer.',\n",
       "   'Bayesian formalisms treat the true reward as a latent variable, and treat observed feedback (often referred to as a proxy reward) as evidence about this latent variable [Hadﬁeld-Menell et al., 2016, 2017b, Woodward et al., 2020, Russell, 2019, Jeon et al., 2020].',\n",
       "   'We build on both Singh et al. [2009] and Jeon et al. [2020] by considering the presence of corrupt feedback, which we emphasize is a key consideration for designing feedback functions.'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': '5cde940de9cc7cac8595e790e397ca56b5316202',\n",
       "   'url': 'https://www.semanticscholar.org/paper/5cde940de9cc7cac8595e790e397ca56b5316202',\n",
       "   'title': 'REALab: An Embedded Perspective on Tampering',\n",
       "   'year': 2020,\n",
       "   'authors': [{'authorId': '2117776492', 'name': 'Ramana Kumar'},\n",
       "    {'authorId': '9960452', 'name': 'Jonathan Uesato'},\n",
       "    {'authorId': '2047148926', 'name': 'Richard Ngo'},\n",
       "    {'authorId': '1868196', 'name': 'Tom Everitt'},\n",
       "    {'authorId': '2578985', 'name': 'Victoria Krakovna'},\n",
       "    {'authorId': '34313265', 'name': 'S. Legg'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['1In our experiments we are consistent with previous work on inverse reinforcement learning and use rθ(ξ) = θ · Φ(ξ), where Φ(ξ) ∈ Rk are known features and θ ∈ Rk are unknown reward weights [4], [9]–[11].',\n",
       "   'Most relevant are [4] — where the authors formalize choice sets in reward learning — and [5] — where the authors experimentally compare different classes of choice set misspecification.',\n",
       "   'We refer to the set of possible demonstrations that a given human can show the robot as the human’s choice set [4], [5].'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '9ea8a61f8fa09244686fc0a9031dc3311ed37c60',\n",
       "   'url': 'https://www.semanticscholar.org/paper/9ea8a61f8fa09244686fc0a9031dc3311ed37c60',\n",
       "   'title': 'I Know What You Meant: Learning Human Objectives by (Under)estimating Their Choice Set',\n",
       "   'year': 2020,\n",
       "   'authors': [{'authorId': '2008860791', 'name': 'Ananth Jonnavittula'},\n",
       "    {'authorId': '2426559', 'name': 'Dylan P. Losey'}]}},\n",
       " {'isInfluential': True,\n",
       "  'contexts': ['A similar principle has been successfully employed in robotics (Jeon et al., 2020; Reddy et al., 2019), and understanding user behavior on web (Azzopardi, 2014; Kosinski et al.',\n",
       "   'General assumption of users trying to maximize their utility proposed in Reddy et al. (2019); Jeon et al. (2020) holds for interactive systems as well Azzopardi (2014).',\n",
       "   'A similar principle has been successfully employed in robotics (Jeon et al., 2020; Reddy et al., 2019), and understanding user behavior on web (Azzopardi, 2014; Kosinski et al., 2013; Wei et al., 2017).',\n",
       "   'Recent work (Leike et al., 2018; Zhang and Dragan, 2019; Jeon et al., 2020) demonstrate impressive results and outline new research direction while modeling user-system interaction using the agent alignment problem (Sutton and Barto, 2018).',\n",
       "   'Jeon et al. (2020) and Reddy et al. (2019) demonstrate how this approach can be applied in the robotics domain.'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': 'b21dcffbc50ce121d54cb3d3931f543e5789f33c',\n",
       "   'url': 'https://www.semanticscholar.org/paper/b21dcffbc50ce121d54cb3d3931f543e5789f33c',\n",
       "   'title': 'Optimizing Interactive Systems via Data-Driven Objectives',\n",
       "   'year': 2020,\n",
       "   'authors': [{'authorId': None, 'name': 'Ziming Li'},\n",
       "    {'authorId': '2887323', 'name': 'A. Grotov'},\n",
       "    {'authorId': '1755651', 'name': 'Julia Kiseleva'},\n",
       "    {'authorId': '1696030', 'name': 'M. de Rijke'},\n",
       "    {'authorId': '143624743', 'name': 'Harrie Oosterhuis'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['…comparisons (Biyik et al. 2019a), general Markov Decision Process (MDP) settings (Katz et al. 2019), online interactive settings (Myers et al. 2023), and settings that combine expert demonstrations or other forms of human feedback with preference queries (Jeon et al. 2020; Biyik et al. 2021).',\n",
       "   '‡ To simplify the notation, we replace Ψ( ξ ) with Ψ , with superscripts and subscripts when needed.'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': '061bfda5b75e03dcb15ee4e94e841c2aeeaeccbf',\n",
       "   'url': 'https://www.semanticscholar.org/paper/061bfda5b75e03dcb15ee4e94e841c2aeeaeccbf',\n",
       "   'title': 'Active preference-based Gaussian process regression for reward learning and optimization',\n",
       "   'year': 2020,\n",
       "   'authors': [{'authorId': '8307674', 'name': 'Erdem Biyik'},\n",
       "    {'authorId': '2163719933', 'name': 'Nicolas Huynh'},\n",
       "    {'authorId': '2275756', 'name': 'Mykel J. Kochenderfer'},\n",
       "    {'authorId': '1779671', 'name': 'Dorsa Sadigh'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['Recent work [48, 62, 158] demonstrates impressive results and outlines a new research direction while modeling user-system interactions using the agent alignment problem [132].',\n",
       "   'The general assumption of users trying to maximize their utility proposed in [48, 107] holds for interactive systems as well [5].',\n",
       "   'A similar principle has been successfully employed in robotics [36, 48, 107] and for understanding user behavior on the web [5, 59, 141].'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '754ab31270e485c4d2d5b6753ce8223bc8cb39cf',\n",
       "   'url': 'https://www.semanticscholar.org/paper/754ab31270e485c4d2d5b6753ce8223bc8cb39cf',\n",
       "   'title': 'Optimizing Interactive Systems with Data-Driven Objectives',\n",
       "   'year': 2018,\n",
       "   'authors': [{'authorId': '2258698077', 'name': 'Ziming Li'},\n",
       "    {'authorId': '2887323', 'name': 'A. Grotov'},\n",
       "    {'authorId': '2244686636', 'name': 'Julia Kiseleva'},\n",
       "    {'authorId': '1696030', 'name': 'M. de Rijke'},\n",
       "    {'authorId': '143624743', 'name': 'Harrie Oosterhuis'}]}},\n",
       " {'isInfluential': True,\n",
       "  'contexts': ['In general, it would be desirable to investigate both settings with more general human models, and to learn how the results generalize to RLHF-variants like DPO (Rafailov et al., 2023), other variants of reward learning (Jeon et al., 2020), and assistance games (Hadfield-Menell et al., 2016).',\n",
       "   'Under the Boltzmann rationality model, we assume the human picks ⃗s with probability where β > 0 is an inverse temperature parameter and σ ( x ) := 1 1+exp( − x ) is the sigmoid function (Bradley & Terry, 1952; Christiano et al., 2017; Jeon et al., 2020).',\n",
       "   'This is an instance of reward-rational (implicit) choice (Jeon et al., 2020), with the function ⃗o (cid:55)→ B ( · | ⃗o ) as the grounding function .',\n",
       "   'RLHF is a special case of reward-rational choice (Jeon et al., 2020), a general framework which also encompasses demonstrations-based inverse reinforcement learning (Ziebart et al., 2008; Ng et al., 2000) and learning from the initial environment state (Shah et al., 2019), and can be seen as a…'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': 'aae77645ecf5b53be3df128c3919c94bcbd02f28',\n",
       "   'url': 'https://www.semanticscholar.org/paper/aae77645ecf5b53be3df128c3919c94bcbd02f28',\n",
       "   'title': 'When Your AIs Deceive You: Challenges with Partial Observability of Human Evaluators in Reward Learning',\n",
       "   'year': 2024,\n",
       "   'authors': [{'authorId': '2287768830', 'name': 'Leon Lang'},\n",
       "    {'authorId': '2287830013', 'name': 'Davis Foote'},\n",
       "    {'authorId': '2273929096', 'name': 'Stuart J. Russell'},\n",
       "    {'authorId': '2064066935', 'name': 'Anca Dragan'},\n",
       "    {'authorId': '2287826196', 'name': 'Erik Jenner'},\n",
       "    {'authorId': '2237427074', 'name': 'Scott Emmons'}]}},\n",
       " {'isInfluential': True,\n",
       "  'contexts': [', by (Jeon et al., 2020), but the focus has been mostly on unimodal feedback.',\n",
       "   'In recent years, preference learning has been an active research topic (Wirth et al., 2017), but integrative work on learning from multiple feedback modalities has been much sparser and is often limited to e.g., a theoretical analysis (Jeon et al., 2020).',\n",
       "   'Jeon et al. (2020) define a formalism that enables integrating various feedback types into a unified framework.',\n",
       "   'There has been research on unifying multiple feedback types into a single framework, e.g., by (Jeon et al., 2020), but the focus has been mostly on unimodal feedback.'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '74e3b120e375e2fa98bbee0241f6ed3eeec75a3c',\n",
       "   'url': 'https://www.semanticscholar.org/paper/74e3b120e375e2fa98bbee0241f6ed3eeec75a3c',\n",
       "   'title': 'Adversarial Imitation Learning with Preferences',\n",
       "   'year': 2023,\n",
       "   'authors': [{'authorId': '2275603011', 'name': 'Aleksandar Taranovic'},\n",
       "    {'authorId': '2811712', 'name': 'A. Kupcsik'},\n",
       "    {'authorId': '1443836920', 'name': 'Niklas Freymuth'},\n",
       "    {'authorId': '26599977', 'name': 'G. Neumann'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['Although this success is recent, incorporating human feedback into reinforcement learning methods via a learned reward model has a deep history in reward learning Christiano et al. (2017); Jeon et al. (2020).',\n",
       "   'Although this success is recent, incorporating human feedback into reinforcement learning methods via a learned reward model has a deep history in reward learning [8, 11].'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '1123d1ff70881729cd9085147603189efe6d4cc6',\n",
       "   'url': 'https://www.semanticscholar.org/paper/1123d1ff70881729cd9085147603189efe6d4cc6',\n",
       "   'title': 'Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining',\n",
       "   'year': 2023,\n",
       "   'authors': [{'authorId': '2304511423', 'name': 'Robin Netzorg'},\n",
       "    {'authorId': '2136085064', 'name': 'Jiaxun Li'},\n",
       "    {'authorId': '46806278', 'name': 'Ting Yu'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['We also adopt the Boltzmann rational model of human behavior [59, 60], which models that the probability of choosing a particular option increases exponentially as its utility, or reward, increases compared to other available options.',\n",
       "   'We also adopt the Boltzmann rational model of human behavior [59, 60], which models that human decisions are exponentially likely with respect to reward.'],\n",
       "  'intents': ['methodology'],\n",
       "  'citingPaper': {'paperId': 'bb6fc9b69cf482083b931df8ec4e5d2388b0a883',\n",
       "   'url': 'https://www.semanticscholar.org/paper/bb6fc9b69cf482083b931df8ec4e5d2388b0a883',\n",
       "   'title': 'Learning Human Contribution Preferences in Collaborative Human-Robot Tasks',\n",
       "   'year': 2023,\n",
       "   'authors': [{'authorId': '2188360881', 'name': 'Michelle D. Zhao'},\n",
       "    {'authorId': '2284770534', 'name': 'Reid G. Simmons'},\n",
       "    {'authorId': '1882027', 'name': 'H. Admoni'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['Jeon et al. [30] propose the framework of reward-rational implicit choice, which interprets human feedback (regardless of its form) as a choice from a set of possibly infinitely many alternatives.',\n",
       "   'As a step in the direction of learning from implicit feedback, Jeon et al. [30] propose the framework of reward-rational implicit choice to learn from explicit and implicit forms of feedback.'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '8fa6b111d810c928bbdc966303a7ea57cce57f49',\n",
       "   'url': 'https://www.semanticscholar.org/paper/8fa6b111d810c928bbdc966303a7ea57cce57f49',\n",
       "   'title': 'On the Challenges and Practices of Reinforcement Learning from Real Human Feedback',\n",
       "   'year': 2023,\n",
       "   'authors': [{'authorId': '2276205601', 'name': 'Timo Kaufmann'},\n",
       "    {'authorId': '2297476419', 'name': 'Sarah Ball'},\n",
       "    {'authorId': '2192962876', 'name': 'Jacob Beck'},\n",
       "    {'authorId': '1691955', 'name': 'Eyke Hüllermeier'},\n",
       "    {'authorId': '2249553408', 'name': 'Frauke Kreuter'}]}},\n",
       " {'isInfluential': True,\n",
       "  'contexts': ['If the speaker’s horizon H is known, we can write a L 1 listener as: Given an instruction, L 1 infers the reward weights that would make such an instruction optimal [13, 14]; given a description, L 1 can recover information about features that were not mentioned [24, 25].',\n",
       "   'Instructions map to speciﬁc actions or trajectories [7, 13]; in our work, “instruction” utterances correspond to the nine actions (Fig 1A).',\n",
       "   'We formalized the challenge of using language to teach an agent to act on our behalf [13, 14, 43, 44, 81] in a bandit setting, introducing the notion of a horizon to reﬂect the agent’s degree of autonomy.',\n",
       "   'This paradigm assumes instructions reﬂect underlying preferences and uses them to infer this latent reward function [13, 14].',\n",
       "   'The horizon quantiﬁes notions of autonomy described in previous work [13, 14, 25].',\n",
       "   'When the state is known, we deﬁne the present utility of an utterance as the expected reward from using the updated policy to choose an action: However, we are interested in settings where agents may need to act autonomously [13, 24, 25].',\n",
       "   'When the desired behavior is known (but the reward function is not), inverse reinforcement learning [IRL, 13, 14, 27, 28, 42, 45, 46] can be used to infer an expert’s reward function from their actions.',\n",
       "   'However, if autonomy is desired [1–3, 13, 14, 38, 43, 44], agents should be equipped to understand descriptions of the world or our preferences [21–25].'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': '5aa85e72597e1cc3b851af007af30124b0ca5b59',\n",
       "   'url': 'https://www.semanticscholar.org/paper/5aa85e72597e1cc3b851af007af30124b0ca5b59',\n",
       "   'title': 'How to talk so your AI will learn: Instructions, descriptions, and autonomy',\n",
       "   'year': 2022,\n",
       "   'authors': [{'authorId': '1976174397', 'name': 'T. Sumers'},\n",
       "    {'authorId': None, 'name': 'Robert D Hawkins'},\n",
       "    {'authorId': '2543534', 'name': 'Mark K. Ho'},\n",
       "    {'authorId': '1799860', 'name': 'T. Griffiths'},\n",
       "    {'authorId': '1397904824', 'name': 'Dylan Hadfield-Menell'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['Instructions map to speciﬁc actions or trajectories [7, 12]; in our work, “instruction” utterances correspond to the nine actions (Fig 1A).',\n",
       "   'When the desired behavior is known (but the reward function is not), inverse reinforcement learning [IRL, 12, 14, 26, 27, 45–47] can be used to infer an expert’s reward function from their actions.'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': '8c206a4a0457dfa388898e3bf07d0385d2fdbd5d',\n",
       "   'url': 'https://www.semanticscholar.org/paper/8c206a4a0457dfa388898e3bf07d0385d2fdbd5d',\n",
       "   'title': 'How to talk so your robot will learn: Instructions, descriptions, and pragmatics',\n",
       "   'year': 2022,\n",
       "   'authors': [{'authorId': '1976174397', 'name': 'T. Sumers'},\n",
       "    {'authorId': None, 'name': 'Robert D Hawkins'},\n",
       "    {'authorId': '2543534', 'name': 'Mark K. Ho'},\n",
       "    {'authorId': '1799860', 'name': 'T. Griffiths'},\n",
       "    {'authorId': '1397904824', 'name': 'Dylan Hadfield-Menell'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': [', 2017) or decision-theoretic treatment (Parikh, 2001; Golland et al., 2010; Jäger, 2012; Franke, 2013; Jeon et al., 2020).',\n",
       "   '…listeners is modeled using an agent-based probabilistic (Rosenberg and Cohen, 1964; Frank and Goodman, 2012; Goodman and Stuhlmüller, 2013; Wang et al., 2016b; Monroe et al., 2017) or decision-theoretic treatment (Parikh, 2001; Golland et al., 2010; Jäger, 2012; Franke, 2013; Jeon et al., 2020).'],\n",
       "  'intents': ['methodology'],\n",
       "  'citingPaper': {'paperId': '80d5684c1e7f5ae34b524ed25dd51f2aa2b613c9',\n",
       "   'url': 'https://www.semanticscholar.org/paper/80d5684c1e7f5ae34b524ed25dd51f2aa2b613c9',\n",
       "   'title': 'Learning Grounded Pragmatic Communication',\n",
       "   'year': 2021,\n",
       "   'authors': [{'authorId': '47070750', 'name': 'Daniel Fried'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['In such direction, several recent contributions deal with inferential social learning to understand how humans think, plan, and act during interactive robot/agent learning [45, 58, 18, 97, 40].',\n",
       "   'A common approach is to formalize human intent via a reward function, and assume that the human will act rationally with regard to the reward function [45, 55].',\n",
       "   'In [45], the authors introduce a formalism exploiting such a framework to interpret different types of human behaviors called the reward-rational choice .'],\n",
       "  'intents': ['background'],\n",
       "  'citingPaper': {'paperId': 'ede60a10f12f4f1a966abd7fc034f5187c4d6b08',\n",
       "   'url': 'https://www.semanticscholar.org/paper/ede60a10f12f4f1a966abd7fc034f5187c4d6b08',\n",
       "   'title': 'Interactive Robot Learning: An Overview',\n",
       "   'year': 2021,\n",
       "   'authors': [{'authorId': '1680828', 'name': 'M. Chetouani'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['There are many types of choices [33], including demonstrations [44, 61, 24], comparisons [60, 57, 13, 50], corrections [5], the state of the world [51], proxy rewards [29], natural language [25], etc.',\n",
       "   'Reward learning [13, 61, 50, 57, 33] attempts to instantiate this by learning a reward model from human feedback, and then using a control algorithm to optimize the learned reward.'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': '6a02153b97823b08283b17f964d8d17ddf3c1ccd',\n",
       "   'url': 'https://www.semanticscholar.org/paper/6a02153b97823b08283b17f964d8d17ddf3c1ccd',\n",
       "   'title': 'Beneﬁts of Assistance over Reward Learning',\n",
       "   'year': 2020,\n",
       "   'authors': [{'authorId': '40947489', 'name': 'Rohin Shah'},\n",
       "    {'authorId': '2056901578', 'name': 'Pedro J Freire'},\n",
       "    {'authorId': '50632861', 'name': 'Rachel Freedman'},\n",
       "    {'authorId': '52510051', 'name': 'Dmitrii Krasheninnikov'},\n",
       "    {'authorId': '2072836382', 'name': 'Lawrence Chan'},\n",
       "    {'authorId': '95358337', 'name': 'Michael Dennis'},\n",
       "    {'authorId': '1689992', 'name': 'P. Abbeel'},\n",
       "    {'authorId': '2745001', 'name': 'A. Dragan'},\n",
       "    {'authorId': '145107462', 'name': 'Stuart J. Russell'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['This model is then used to construct a probabilistic interpretation of that feedback and their counterfactuals [30, 45, 46, 62], which is widely adopted in fields like psychology [4, 24, 25, 48], economics [12, 46, 49, 53], and artificial intelligence [11, 23, 30, 37].',\n",
       "   'The teacher can intervene and modify the robot’s motion, producing a corrected trajectory that is assumed to be more optimal with respect to the hidden task objectives [3, 30].'],\n",
       "  'intents': ['background', 'methodology'],\n",
       "  'citingPaper': {'paperId': '1cbb8b5bba10c3fabf839b687894e23efaa4deaa',\n",
       "   'url': 'https://www.semanticscholar.org/paper/1cbb8b5bba10c3fabf839b687894e23efaa4deaa',\n",
       "   'title': 'Toward Measuring the Effect of Robot Competency on Human Kinesthetic Feedback in Long-Term Task Learning',\n",
       "   'year': None,\n",
       "   'authors': [{'authorId': '2294170604', 'name': 'Shuangge Wang'},\n",
       "    {'authorId': '2279585086', 'name': 'Brian Scassellati'},\n",
       "    {'authorId': '2286471044', 'name': 'Tesca Fitzgerald'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['Traditionally, Inverse RL maps from policy to reward, but I will lean on the Reward Rational Implicit Choice framework (Jeon et al., 2020) and consider the broader view of mapping arbitrary (possibly implicit) choices to rewards: This figure decomposes the IRL arrow from the first figure into three…',\n",
       "   'There has been a trend in recent work toward defining a trajectory-wise reward function/model R : τ (cid:55)→ r (Ziegler et al., 2019; Jeon et al., 2020; Stiennon et al., 2020; Ouyang et al., 2022).',\n",
       "   '…some context C , human preference is expressed by choosing one or more options from a choice set X , which may be a set of trajectories, natural language utterances, demonstrations, or any other modality that provides signal about human preference (see Jeon et al. (2020) for other possibilities).'],\n",
       "  'intents': [],\n",
       "  'citingPaper': {'paperId': 'c35fb9c7f10a60488ded4a1bb520e68c9ec957a5',\n",
       "   'url': 'https://www.semanticscholar.org/paper/c35fb9c7f10a60488ded4a1bb520e68c9ec957a5',\n",
       "   'title': 'Failure Modes of Learning Reward Models for LLMs and other Sequence Models',\n",
       "   'year': None,\n",
       "   'authors': [{'authorId': '32305445', 'name': 'Silviu Pitis'}]}},\n",
       " {'isInfluential': False,\n",
       "  'contexts': ['…& Terry, 1952; Rajkumar & Agarwal, 2014; Christiano et al., 2017), which assumes humans are Boltzmann rational (Luce, 1959; Ziebart et al., 2010; Jeon et al., 2020)—when people express their preferences, their likelihood of choosing a particular option is proportional to the exponentiated value…',\n",
       "   'Under the current preference learning paradigm, humans are modeled as Boltzmann rational (Jeon et al., 2020), which implies that as annotators provide preference comparisons, their probability of choosing a particular option is proportional to the exponentiated value or reward that they associate…',\n",
       "   'Prior research has focused on assigning it a value of 1 (Christiano et al., 2017; Ibarz et al., 2018) or another fixed value for all provided preferences (Shah et al., 2019; Bıyık et al., 2020; Jeon et al., 2020; Lee et al., 2020).'],\n",
       "  'intents': [],\n",
       "  'citingPaper': {'paperId': '58ebc7f497eb8770bdd53ba247710a2ecee88950',\n",
       "   'url': 'https://www.semanticscholar.org/paper/58ebc7f497eb8770bdd53ba247710a2ecee88950',\n",
       "   'title': 'Scalable Oversight by Accounting for Unreliable Feedback',\n",
       "   'year': None,\n",
       "   'authors': [{'authorId': '2290017375', 'name': 'Shivam Singhal'},\n",
       "    {'authorId': '114339785', 'name': 'Cassidy Laidlaw'},\n",
       "    {'authorId': '2064066935', 'name': 'Anca Dragan'}]}}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d8ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
