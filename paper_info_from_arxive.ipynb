{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get markdown formated details of a paper from arxive url/id\n",
    "- From a list of arxive urls I want to get a formated previous so I can past it in Notion.\n",
    "- Although Notion will recognize some md comments when you paste them such as \"#\" for (sub)sections\n",
    "-  It will not recognize other markdown things such as a expandable header. \n",
    "    - Therefore, past the output in a .md file and copy it form there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_arxiv_info(arxiv_id):\n",
    "    # print(\"arxiv id is \", arxiv_id)\n",
    "    \n",
    "    # Search for papers by arXiv ID\n",
    "    search = arxiv.Search(id_list=[arxiv_id])\n",
    "    paper = next(search.results(), None)  # Fetch the first result\n",
    "    \n",
    "    if paper:\n",
    "        # Extracting the necessary information\n",
    "        title = paper.title\n",
    "        abstract = paper.summary\n",
    "        authors = \", \".join(author.name for author in paper.authors)\n",
    "        year = paper.published.year\n",
    "        url = paper.entry_id  # Fetch the URL\n",
    "        \n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"authors\": authors,\n",
    "            \"year\": year,\n",
    "            \"url\": url  # Return the URL\n",
    "        }\n",
    "    else:\n",
    "        return \"No paper found with that arXiv ID.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_30581/2987022110.py:6: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  paper = next(search.results(), None)  # Fetch the first result\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "def format_paper_info(info):\n",
    "    # info is a dict with fields: title, abstract, authors, year, url\n",
    "    authors = info['authors'].split(\",\")[0].split(\" \")  # Only the first author (for brevity)\n",
    "    if len(authors) > 1:\n",
    "        # use the initials for first names and full last name\n",
    "        authors = \" \".join([f\"{name[0]}.\" if i < len(authors) - 1 else name for i, name in enumerate(authors)])\n",
    "    outp_str = f\"###  [{info['title']} - {authors} ({info['year']})]({info['url']})\\n\\n\"\n",
    "    outp_str += \"<details open>\\n<summary>Abstract</summary>\\n\\n\"\n",
    "    outp_str += f\"> {info['abstract']}\\n\\n\"\n",
    "    outp_str += \"</details> \\n\\n\"\n",
    "    return outp_str\n",
    "\n",
    "def process_arxivurl_list(arxiv_url_str):\n",
    "    arxiv_urls_og = arxiv_url_str.split(\"\\n\")\n",
    "    arxiv_urls_og = [url.strip() for url in arxiv_urls_og if len(url.strip()) > 0]\n",
    "    arxiv_urls = [item for item in arxiv_urls_og if \"arxiv.org\" in item]\n",
    "    arxiv_ids = [url.split(\"/\")[-1].replace(\".pdf\", \"\").strip() for url in arxiv_urls]\n",
    "    non_arxive_urls = [url for url in arxiv_urls_og if url not in arxiv_urls]\n",
    "    formatted_info = \"\"\n",
    "\n",
    "    if len(non_arxive_urls) > 0:\n",
    "        formatted_info += \"\\n ### Non arXiv URLs: \\n\\n - \"\n",
    "        formatted_info += \"\\n - \".join(non_arxive_urls)\n",
    "    \n",
    "    formatted_info += \"\\n ## ArXiv items: \\n\\n\"\n",
    "    for arxiv_id in tqdm.tqdm(arxiv_ids):\n",
    "        info = fetch_arxiv_info(arxiv_id)\n",
    "        formatted_info += format_paper_info(info)\n",
    "\n",
    "    # create a temp.md file and append the formatted_info to it\n",
    "    with open(\"temp.md\", \"w\") as f:\n",
    "        f.write(formatted_info)\n",
    "        f.write(\"\\n\\n---\\n\\n\")\n",
    "\n",
    "    return formatted_info\n",
    "\n",
    "\n",
    "\n",
    "arxiv_url_str = \"\"\"\n",
    "\n",
    "https://arxiv.org/abs/2401.06102\n",
    "https://gist.github.com/pierrejoubert73/902cc94d79424356a8d20be2b382e1ab\n",
    "\n",
    "https://arxiv.org/pdf/2403.15419.pdf\n",
    "\"\"\"\n",
    "\n",
    "formatted_info = process_arxivurl_list(arxiv_url_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       " ### Non arXiv URLs: \n",
       "\n",
       " - https://gist.github.com/pierrejoubert73/902cc94d79424356a8d20be2b382e1ab\n",
       " ## ArXiv items: \n",
       "\n",
       "###  [Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models - A. Ghandeharioun (2024)](http://arxiv.org/abs/2401.06102v2)\n",
       "\n",
       "<details open>\n",
       "<summary>Abstract</summary>\n",
       "\n",
       "> Inspecting the information encoded in hidden representations of large\n",
       "language models (LLMs) can explain models' behavior and verify their alignment\n",
       "with human values. Given the capabilities of LLMs in generating\n",
       "human-understandable text, we propose leveraging the model itself to explain\n",
       "its internal representations in natural language. We introduce a framework\n",
       "called Patchscopes and show how it can be used to answer a wide range of\n",
       "questions about an LLM's computation. We show that prior interpretability\n",
       "methods based on projecting representations into the vocabulary space and\n",
       "intervening on the LLM computation can be viewed as instances of this\n",
       "framework. Moreover, several of their shortcomings such as failure in\n",
       "inspecting early layers or lack of expressivity can be mitigated by\n",
       "Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also\n",
       "opens up new possibilities such as using a more capable model to explain the\n",
       "representations of a smaller model, and unlocks new applications such as\n",
       "self-correction in multi-hop reasoning.\n",
       "\n",
       "</details> \n",
       "\n",
       "###  [Attention is all you need for boosting graph convolutional neural network - Y. Wu (2024)](http://arxiv.org/abs/2403.15419v1)\n",
       "\n",
       "<details open>\n",
       "<summary>Abstract</summary>\n",
       "\n",
       "> Graph Convolutional Neural Networks (GCNs) possess strong capabilities for\n",
       "processing graph data in non-grid domains. They can capture the topological\n",
       "logical structure and node features in graphs and integrate them into nodes'\n",
       "final representations. GCNs have been extensively studied in various fields,\n",
       "such as recommendation systems, social networks, and protein molecular\n",
       "structures. With the increasing application of graph neural networks, research\n",
       "has focused on improving their performance while compressing their size. In\n",
       "this work, a plug-in module named Graph Knowledge Enhancement and Distillation\n",
       "Module (GKEDM) is proposed. GKEDM can enhance node representations and improve\n",
       "the performance of GCNs by extracting and aggregating graph information via\n",
       "multi-head attention mechanism. Furthermore, GKEDM can serve as an auxiliary\n",
       "transferor for knowledge distillation. With a specially designed attention\n",
       "distillation method, GKEDM can distill the knowledge of large teacher models\n",
       "into high-performance and compact student models. Experiments on multiple\n",
       "datasets demonstrate that GKEDM can significantly improve the performance of\n",
       "various GCNs with minimal overhead. Furthermore, it can efficiently transfer\n",
       "distilled knowledge from large teacher networks to small student networks via\n",
       "attention distillation.\n",
       "\n",
       "</details> \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown(formatted_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
